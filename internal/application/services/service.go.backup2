package services

import (
	"context"
	"crypto/sha256"
	"fmt"
	"log"
	"sort"
	"strings"
	"time"

	"k8s.io/klog/v2"

	"netguard-pg-backend/internal/application/validation"
	"netguard-pg-backend/internal/domain/models"
	"netguard-pg-backend/internal/domain/ports"
	"netguard-pg-backend/internal/k8s/apis/netguard/v1beta1"
	"netguard-pg-backend/internal/sync/interfaces"
	"netguard-pg-backend/internal/sync/types"

	"github.com/pkg/errors"
)

// NetguardService provides operations for managing netguard resources
type NetguardService struct {
	registry              ports.Registry
	conditionManager      *ConditionManager
	syncManager           interfaces.SyncManager
	networkService        *NetworkService
	networkBindingService *NetworkBindingService
}

// NewNetguardService creates a new NetguardService
func NewNetguardService(registry ports.Registry, syncManager interfaces.SyncManager) *NetguardService {
	s := &NetguardService{
		registry:    registry,
		syncManager: syncManager,
	}
	s.conditionManager = NewConditionManager(registry, s)
	s.networkService = NewNetworkService(registry, syncManager)
	s.networkBindingService = NewNetworkBindingService(registry, s.networkService, syncManager)
	return s
}

// GetServices returns all services
func (s *NetguardService) GetServices(ctx context.Context, scope ports.Scope) ([]models.Service, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var services []models.Service
	err = reader.ListServices(ctx, func(service models.Service) error {
		services = append(services, service)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list services")
	}
	return services, nil
}

// GetAddressGroups returns all address groups
func (s *NetguardService) GetAddressGroups(ctx context.Context, scope ports.Scope) ([]models.AddressGroup, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var addressGroups []models.AddressGroup
	err = reader.ListAddressGroups(ctx, func(addressGroup models.AddressGroup) error {
		addressGroups = append(addressGroups, addressGroup)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list address groups")
	}
	return addressGroups, nil
}

// GetAddressGroupBindings returns all address group bindings
func (s *NetguardService) GetAddressGroupBindings(ctx context.Context, scope ports.Scope) ([]models.AddressGroupBinding, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var bindings []models.AddressGroupBinding
	err = reader.ListAddressGroupBindings(ctx, func(binding models.AddressGroupBinding) error {
		bindings = append(bindings, binding)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list address group bindings")
	}
	return bindings, nil
}

// GetAddressGroupPortMappings returns all address group port mappings
func (s *NetguardService) GetAddressGroupPortMappings(ctx context.Context, scope ports.Scope) ([]models.AddressGroupPortMapping, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var mappings []models.AddressGroupPortMapping
	err = reader.ListAddressGroupPortMappings(ctx, func(mapping models.AddressGroupPortMapping) error {
		mappings = append(mappings, mapping)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list address group port mappings")
	}
	return mappings, nil
}

// GetRuleS2S returns all rule s2s
func (s *NetguardService) GetRuleS2S(ctx context.Context, scope ports.Scope) ([]models.RuleS2S, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var rules []models.RuleS2S
	err = reader.ListRuleS2S(ctx, func(rule models.RuleS2S) error {
		rules = append(rules, rule)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list rule s2s")
	}
	return rules, nil
}

// GetServiceAliases returns all service aliases
func (s *NetguardService) GetServiceAliases(ctx context.Context, scope ports.Scope) ([]models.ServiceAlias, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var aliases []models.ServiceAlias
	err = reader.ListServiceAliases(ctx, func(alias models.ServiceAlias) error {
		aliases = append(aliases, alias)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list service aliases")
	}
	return aliases, nil
}

// GetAddressGroupBindingPolicies returns all address group binding policies
func (s *NetguardService) GetAddressGroupBindingPolicies(ctx context.Context, scope ports.Scope) ([]models.AddressGroupBindingPolicy, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var policies []models.AddressGroupBindingPolicy
	err = reader.ListAddressGroupBindingPolicies(ctx, func(policy models.AddressGroupBindingPolicy) error {
		policies = append(policies, policy)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list address group binding policies")
	}
	return policies, nil
}

// GetNetworks returns all networks
func (s *NetguardService) GetNetworks(ctx context.Context, scope ports.Scope) ([]models.Network, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var networks []models.Network
	err = reader.ListNetworks(ctx, func(network models.Network) error {
		networks = append(networks, network)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list networks")
	}
	return networks, nil
}

// GetNetworkBindings returns all network bindings
func (s *NetguardService) GetNetworkBindings(ctx context.Context, scope ports.Scope) ([]models.NetworkBinding, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var bindings []models.NetworkBinding
	err = reader.ListNetworkBindings(ctx, func(binding models.NetworkBinding) error {
		bindings = append(bindings, binding)
		return nil
	}, scope)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list network bindings")
	}
	return bindings, nil
}

// CreateService создает новый сервис
func (s *NetguardService) CreateService(ctx context.Context, service models.Service) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	serviceValidator := validator.GetServiceValidator()

	// Валидируем сервис перед созданием
	if err := serviceValidator.ValidateForCreation(ctx, service); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncServices(ctx, []models.Service{service}, ports.NewResourceIdentifierScope(service.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to create service")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	// Используем универсальную функцию
	s.processConditionsIfNeeded(ctx, &service, models.SyncOpUpsert)
	return nil
}

// CreateAddressGroup создает новую группу адресов
func (s *NetguardService) CreateAddressGroup(ctx context.Context, addressGroup models.AddressGroup) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	addressGroupValidator := validator.GetAddressGroupValidator()

	// Валидируем группу адресов перед созданием
	if err := addressGroupValidator.ValidateForCreation(ctx, addressGroup); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroups(ctx, []models.AddressGroup{addressGroup}, ports.NewResourceIdentifierScope(addressGroup.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to create address group")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Синхронизация с sgroups после успешного создания в БД
	s.syncAddressGroupsWithSGroups(ctx, []models.AddressGroup{addressGroup}, types.SyncOperationUpsert)

	// Используем универсальную функцию
	s.processConditionsIfNeeded(ctx, &addressGroup, models.SyncOpUpsert)
	return nil
}

// UpdateService обновляет существующий сервис
func (s *NetguardService) UpdateService(ctx context.Context, service models.Service) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Получаем старую версию сервиса
	oldService, err := reader.GetServiceByID(ctx, service.ResourceIdentifier)
	if err != nil {
		return errors.Wrap(err, "failed to get existing service")
	}

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	serviceValidator := validator.GetServiceValidator()

	// Валидируем сервис перед обновлением
	if err := serviceValidator.ValidateForUpdate(ctx, *oldService, service); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncServices(ctx, []models.Service{service}, ports.NewResourceIdentifierScope(service.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to update service")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	// Используем универсальную функцию
	s.processConditionsIfNeeded(ctx, &service, models.SyncOpUpsert)
	return nil
}

// UpdateAddressGroup обновляет существующую группу адресов
func (s *NetguardService) UpdateAddressGroup(ctx context.Context, addressGroup models.AddressGroup) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Получаем старую версию группы адресов
	oldAddressGroup, err := reader.GetAddressGroupByID(ctx, addressGroup.ResourceIdentifier)
	if err != nil {
		return errors.Wrap(err, "failed to get existing address group")
	}

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	addressGroupValidator := validator.GetAddressGroupValidator()

	// Валидируем группу адресов перед обновлением
	if err := addressGroupValidator.ValidateForUpdate(ctx, *oldAddressGroup, addressGroup); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroups(ctx, []models.AddressGroup{addressGroup}, ports.NewResourceIdentifierScope(addressGroup.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to update address group")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Синхронизация с sgroups после успешного обновления в БД
	s.syncAddressGroupsWithSGroups(ctx, []models.AddressGroup{addressGroup}, types.SyncOperationUpsert)

	// Используем универсальную функцию
	s.processConditionsIfNeeded(ctx, &addressGroup, models.SyncOpUpsert)
	return nil
}

// CreateAddressGroupBinding создает новую привязку группы адресов
func (s *NetguardService) CreateAddressGroupBinding(ctx context.Context, binding models.AddressGroupBinding) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	bindingValidator := validator.GetAddressGroupBindingValidator()

	// Валидируем привязку перед созданием
	if err := bindingValidator.ValidateForCreation(ctx, &binding); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroupBindings(ctx, []models.AddressGroupBinding{binding}, ports.NewResourceIdentifierScope(binding.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to create address group binding")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	// Обработка conditions
	s.conditionManager.ProcessAddressGroupBindingConditions(ctx, &binding)
	if err := s.conditionManager.saveResourceConditions(ctx, &binding); err != nil {
		return errors.Wrap(err, "failed to save address group binding conditions")
	}
	return nil
}

// UpdateAddressGroupBinding обновляет существующую привязку группы адресов
func (s *NetguardService) UpdateAddressGroupBinding(ctx context.Context, binding models.AddressGroupBinding) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Получаем старую версию привязки
	oldBinding, err := reader.GetAddressGroupBindingByID(ctx, binding.ResourceIdentifier)
	if err != nil {
		return errors.Wrap(err, "failed to get existing address group binding")
	}

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	bindingValidator := validator.GetAddressGroupBindingValidator()

	// Валидируем привязку перед обновлением
	if err := bindingValidator.ValidateForUpdate(ctx, *oldBinding, &binding); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroupBindings(ctx, []models.AddressGroupBinding{binding}, ports.NewResourceIdentifierScope(binding.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to update address group binding")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	s.conditionManager.ProcessAddressGroupBindingConditions(ctx, &binding)
	if err := s.conditionManager.saveResourceConditions(ctx, &binding); err != nil {
		return errors.Wrap(err, "failed to save address group binding conditions")
	}
	return nil
}

// Sync выполняет синхронизацию с указанной операцией и субъектом
func (s *NetguardService) Sync(ctx context.Context, syncOp models.SyncOp, subject interface{}) error {
	klog.Infof("🔥 DEBUG: MAIN Sync called with syncOp=%s, subject type=%T", syncOp, subject)
	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// Обработка разных типов субъектов
	klog.Infof("🔥 DEBUG: MAIN Sync starting switch statement for type %T", subject)
	switch v := subject.(type) {
	case []models.Service:
		klog.Infof("🔥 DEBUG: MAIN Sync matched []models.Service case with %d services, syncOp=%s", len(v), syncOp)
		for i, service := range v {
			klog.Infof("🔥 DEBUG: MAIN Sync Service[%d]: %s, ports=%d", i, service.Key(), len(service.IngressPorts))
		}
		if err := s.syncServices(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// Используем универсальную функцию
		s.processConditionsIfNeeded(ctx, v, syncOp)
		return nil
	case []models.AddressGroup:
		if err := s.syncAddressGroups(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// Используем универсальную функцию
		s.processConditionsIfNeeded(ctx, v, syncOp)
		return nil
	case []models.AddressGroupBinding:
		if err := s.syncAddressGroupBindings(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// Используем универсальную функцию
		s.processConditionsIfNeeded(ctx, v, syncOp)
		return nil
	case []models.AddressGroupPortMapping:
		if err := s.syncAddressGroupPortMappings(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// Commit транзакции
		if err := writer.Commit(); err != nil {
			return errors.Wrap(err, "failed to commit")
		}
		// Используем универсальную функцию
		s.processConditionsIfNeeded(ctx, v, syncOp)
		return nil
	case []models.RuleS2S:
		if err := s.syncRuleS2S(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// Используем универсальную функцию
		s.processConditionsIfNeeded(ctx, v, syncOp)
		return nil
	case []models.ServiceAlias:
		if err := s.syncServiceAliases(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// Используем универсальную функцию
		s.processConditionsIfNeeded(ctx, v, syncOp)
		return nil
	case []models.AddressGroupBindingPolicy:
		if err := s.syncAddressGroupBindingPolicies(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// Используем универсальную функцию
		s.processConditionsIfNeeded(ctx, v, syncOp)
		return nil
	case []models.Network:
		if err := s.syncNetworks(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// NOTE: We do NOT call processConditionsIfNeeded here because syncNetworks
		// already handles conditions processing with sgroups sync results
		return nil
	case []models.NetworkBinding:
		if err := s.syncNetworkBindings(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// NOTE: We do NOT call processConditionsIfNeeded here because syncNetworkBindings
		// already handles condition processing within the same transaction.
		// Calling it here would create a separate commit that overwrites our atomic changes.
		return nil
	case []models.IEAgAgRule:
		klog.Infof("🔥 DEBUG: DANGER! General Sync processing %d IEAgAgRules bypassing port aggregation!", len(v))
		for i, rule := range v {
			klog.Infof("🔥 DEBUG: IEAgAgRule[%d]: %s, ports: %s", i, rule.Key(), rule.Ports[0].Destination)
		}
		klog.Infof("🔥 DEBUG: STACK TRACE: This bypasses port aggregation logic!")
		if err := s.syncIEAgAgRules(ctx, writer, v, syncOp); err != nil {
			return err
		}
		// NOTE: syncIEAgAgRules already handles sgroups sync and conditions processing
		return nil
	default:
		return errors.New("unsupported subject type")
	}
}

// processConditionsIfNeeded обрабатывает conditions только для не-удаления операций
func (s *NetguardService) processConditionsIfNeeded(ctx context.Context, subject interface{}, syncOp models.SyncOp) {
	// Пропускаем обработку conditions для операций удаления
	if syncOp == models.SyncOpDelete {
		return
	}

	switch v := subject.(type) {
	case []models.Service:
		for i := range v {
			s.conditionManager.ProcessServiceConditions(ctx, &v[i])
			if err := s.conditionManager.saveResourceConditions(ctx, &v[i]); err != nil {
				log.Printf("Failed to save service conditions for %s: %v", v[i].Key(), err)
			}
		}
	case []models.AddressGroup:
		for i := range v {
			s.conditionManager.ProcessAddressGroupConditions(ctx, &v[i])
			if err := s.conditionManager.saveResourceConditions(ctx, &v[i]); err != nil {
				log.Printf("Failed to save address group conditions for %s: %v", v[i].Key(), err)
			}
		}
	case []models.AddressGroupBinding:
		for i := range v {
			s.conditionManager.ProcessAddressGroupBindingConditions(ctx, &v[i])
			if err := s.conditionManager.saveResourceConditions(ctx, &v[i]); err != nil {
				log.Printf("Failed to save address group binding conditions for %s: %v", v[i].Key(), err)
			}
		}
	case []models.AddressGroupPortMapping:
		for i := range v {
			s.conditionManager.ProcessAddressGroupPortMappingConditions(ctx, &v[i])
			if err := s.conditionManager.saveResourceConditions(ctx, &v[i]); err != nil {
				log.Printf("Failed to save address group port mapping conditions for %s: %v", v[i].Key(), err)
			}
		}
	case []models.RuleS2S:
		for i := range v {
			s.conditionManager.ProcessRuleS2SConditions(ctx, &v[i])
			if err := s.conditionManager.saveResourceConditions(ctx, &v[i]); err != nil {
				log.Printf("Failed to save rule s2s conditions for %s: %v", v[i].Key(), err)
			}
		}
	case []models.ServiceAlias:
		for i := range v {
			s.conditionManager.ProcessServiceAliasConditions(ctx, &v[i])
			if err := s.conditionManager.saveResourceConditions(ctx, &v[i]); err != nil {
				log.Printf("Failed to save service alias conditions for %s: %v", v[i].Key(), err)
			}
		}
	case []models.AddressGroupBindingPolicy:
		for i := range v {
			s.conditionManager.ProcessAddressGroupBindingPolicyConditions(ctx, &v[i])
			if err := s.conditionManager.saveResourceConditions(ctx, &v[i]); err != nil {
				log.Printf("Failed to save address group binding policy conditions for %s: %v", v[i].Key(), err)
			}
		}
	case []models.Network:
		// ВАЖНО: Для Network не используем processConditionsIfNeeded!
		// Условия для Network обрабатываются специально в syncNetworks/SyncNetworks
		// с учетом результата синхронизации sgroups. Это предотвращает race condition
		// и перезапись правильных условий неправильными.
		return
	case []models.NetworkBinding:
		for i := range v {
			s.conditionManager.ProcessNetworkBindingConditions(ctx, &v[i])
			if err := s.conditionManager.saveResourceConditions(ctx, &v[i]); err != nil {
				log.Printf("Failed to save network binding conditions for %s: %v", v[i].Key(), err)
			}
		}
	case *models.AddressGroupPortMapping:
		s.conditionManager.ProcessAddressGroupPortMappingConditions(ctx, v)
		if err := s.conditionManager.saveResourceConditions(ctx, v); err != nil {
			log.Printf("Failed to save address group port mapping conditions for %s: %v", v.Key(), err)
		}
	default:
		// Unknown subject type - no conditions processing needed
	}
}

// CreateAddressGroupPortMapping создает новый маппинг портов группы адресов
func (s *NetguardService) CreateAddressGroupPortMapping(ctx context.Context, mapping models.AddressGroupPortMapping) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	mappingValidator := validator.GetAddressGroupPortMappingValidator()

	// Валидируем маппинг перед созданием
	if err := mappingValidator.ValidateForCreation(ctx, mapping); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroupPortMappings(ctx, []models.AddressGroupPortMapping{mapping}, ports.NewResourceIdentifierScope(mapping.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to create address group port mapping")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	// Используем универсальную функцию
	s.processConditionsIfNeeded(ctx, &mapping, models.SyncOpUpsert)
	return nil
}

// UpdateAddressGroupPortMapping обновляет существующий маппинг портов группы адресов
func (s *NetguardService) UpdateAddressGroupPortMapping(ctx context.Context, mapping models.AddressGroupPortMapping) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Получаем старую версию маппинга
	oldMapping, err := reader.GetAddressGroupPortMappingByID(ctx, mapping.ResourceIdentifier)
	if err != nil {
		return errors.Wrap(err, "failed to get existing address group port mapping")
	}

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	mappingValidator := validator.GetAddressGroupPortMappingValidator()

	// Валидируем маппинг перед обновлением
	if err := mappingValidator.ValidateForUpdate(ctx, *oldMapping, mapping); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroupPortMappings(ctx, []models.AddressGroupPortMapping{mapping}, ports.NewResourceIdentifierScope(mapping.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to update address group port mapping")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	// Используем универсальную функцию
	s.processConditionsIfNeeded(ctx, &mapping, models.SyncOpUpsert)
	return nil
}

// syncServices синхронизирует сервисы с указанной операцией
func (s *NetguardService) syncServices(ctx context.Context, writer ports.Writer, services []models.Service, syncOp models.SyncOp) error {
	klog.Infof("🔥 DEBUG: syncServices called with %d services, syncOp=%s", len(services), syncOp)
	for i, service := range services {
		klog.Infof("🔥 DEBUG: Service[%d]: %s, ports=%d", i, service.Key(), len(service.IngressPorts))
		for j, port := range service.IngressPorts {
			klog.Infof("🔥 DEBUG: Service[%d].IngressPorts[%d]: %s/%s", i, j, port.Port, port.Protocol)
		}
	}
	// Get reader for validation and comparison
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		validator := validation.NewDependencyValidator(reader)
		serviceValidator := validator.GetServiceValidator()

		for _, service := range services {
			existingService, err := reader.GetServiceByID(ctx, service.ResourceIdentifier)
			if err == nil && syncOp != models.SyncOpDelete {
				// Сервис существует - используем ValidateForUpdate
				if err := serviceValidator.ValidateForUpdate(ctx, *existingService, service); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound && syncOp != models.SyncOpDelete {
				// Сервис новый - используем ValidateForCreation
				if err := serviceValidator.ValidateForCreation(ctx, service); err != nil {
					return err
				}
			} else if err != nil && err != ports.ErrNotFound {
				// Произошла другая ошибка
				return errors.Wrap(err, "failed to get service")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if syncOp == models.SyncOpFullSync {
		// При операции FullSync используем пустую область видимости,
		// чтобы удалить все сервисы, а затем добавить только новые
		scope = ports.EmptyScope{}
	} else if len(services) > 0 {
		var ids []models.ResourceIdentifier
		for _, service := range services {
			ids = append(ids, service.ResourceIdentifier)
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Если это удаление, используем DeleteServicesByIDs для корректного каскадного удаления
	if syncOp == models.SyncOpDelete {
		// Собираем ID сервисов
		var ids []models.ResourceIdentifier
		for _, service := range services {
			ids = append(ids, service.ResourceIdentifier)
		}

		// Используем DeleteServicesByIDs для каскадного удаления сервисов и связанных ресурсов
		return s.DeleteServicesByIDs(ctx, ids)
	}

	// Выполнение операции с указанной опцией для не-удаления
	if err := writer.SyncServices(ctx, services, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync services")
	}

	// Если это не удаление, обновляем связанные ресурсы
	var allNewIEAgAgRules []models.IEAgAgRule
	if syncOp != models.SyncOpDelete {
		// OPTIMIZATION: Only update IEAgAgRules for services with changed ports
		var servicesWithPortDiffs []ServicePortDiff
		for _, service := range services {
			// Get existing service for comparison using the original reader (sees old state)
			existingService, err := reader.GetServiceByID(ctx, service.ResourceIdentifier)
			if err != nil {
				// New service - add to update list
				log.Printf("syncServices: Service %s is new, will update related IEAgAgRules", service.Key())
				servicePortDiff := ServicePortDiff{
					ServiceID:    service.ResourceIdentifier,
					AddedPorts:   service.IngressPorts,   // все порты считаются добавленными
					RemovedPorts: []models.IngressPort{}, // ничего не удалено
					IsNewService: true,
				}
				servicesWithPortDiffs = append(servicesWithPortDiffs, servicePortDiff)
				continue
			}

			// Compare ports between old and new version using detailed diff
			addedPorts, removedPorts := CompareServicePorts(existingService.IngressPorts, service.IngressPorts)
			if len(addedPorts) > 0 || len(removedPorts) > 0 {
				servicePortDiff := ServicePortDiff{
					ServiceID:    service.ResourceIdentifier,
					AddedPorts:   addedPorts,
					RemovedPorts: removedPorts,
					IsNewService: false,
				}
				servicesWithPortDiffs = append(servicesWithPortDiffs, servicePortDiff)
				log.Printf("syncServices: Service %s has changed ports, added: %v, removed: %v",
					service.Key(), addedPorts, removedPorts)
			} else {
				log.Printf("syncServices: Service %s ports unchanged, skipping IEAgAgRule update", service.Key())
			}
		}

		// Update IEAgAgRules and Port Mapping only for services with changed ports
		if len(servicesWithPortDiffs) > 0 {
			log.Printf("syncServices: Found %d services with changed ports, updating related IEAgAgRules", len(servicesWithPortDiffs))

			// Get reader that can see changes in current transaction
			txReader, err := s.registry.ReaderFromWriter(ctx, writer)
			if err != nil {
				return errors.Wrap(err, "failed to get transaction reader")
			}
			defer txReader.Close()

			// 1. Обновляем IE AG AG правила
			// Находим все RuleS2S, которые ссылаются на сервисы с измененными портами
			var changedServiceIDs []models.ResourceIdentifier
			for _, portDiff := range servicesWithPortDiffs {
				changedServiceIDs = append(changedServiceIDs, portDiff.ServiceID)
			}
			affectedRules, err := s.findRuleS2SForServicesWithReader(ctx, txReader, changedServiceIDs)
			if err != nil {
				return errors.Wrap(err, "failed to find affected RuleS2S")
			}

			log.Printf("syncServices: Found %d RuleS2S affected by port changes", len(affectedRules))

			// Обновляем IE AG AG правила для затронутых RuleS2S, используя reader из транзакции
			// Используем версию с агрегацией портов для правильного объединения
			if len(affectedRules) > 0 {
				klog.Infof("🔥 DEBUG: syncServices calling updateIEAgAgRulesForRuleS2SWithReader with %d rules", len(affectedRules))
				if err = s.updateIEAgAgRulesForRuleS2SWithReader(ctx, writer, txReader, affectedRules, models.SyncOpFullSync); err != nil {
					return errors.Wrap(err, "failed to update IEAgAgRules for affected RuleS2S")
				}
				klog.Infof("🔥 DEBUG: syncServices completed updateIEAgAgRulesForRuleS2SWithReader successfully")

				// Get all IEAgAgRules from DB for conditions processing (after aggregation)
				err = txReader.ListIEAgAgRules(ctx, func(ieRule models.IEAgAgRule) error {
					allNewIEAgAgRules = append(allNewIEAgAgRules, ieRule)
					return nil
				}, nil)
				if err != nil {
					return errors.Wrap(err, "failed to list all IEAgAgRules for conditions processing")
				}
				log.Printf("syncServices: Found %d IEAgAgRules for conditions processing", len(allNewIEAgAgRules))
			}

			// 2. Обновляем Port Mapping только для сервисов с измененными портами
			// Находим все привязки AddressGroupBinding для сервисов с измененными портами
			var bindings []models.AddressGroupBinding
			err = txReader.ListAddressGroupBindings(ctx, func(binding models.AddressGroupBinding) error {
				for _, serviceID := range changedServiceIDs {
					if binding.ServiceRef.Key() == serviceID.Key() {
						bindings = append(bindings, binding)
						break
					}
				}
				return nil
			}, nil)

			if err != nil {
				return errors.Wrap(err, "failed to list address group bindings")
			}

			// Обновляем Port Mapping для каждой привязки, используя reader из транзакции
			for _, binding := range bindings {
				if err := s.SyncAddressGroupPortMappingsWithWriterAndReader(ctx, writer, txReader, binding, models.SyncOpFullSync); err != nil {
					return errors.Wrapf(err, "failed to sync port mapping for binding %s", binding.Key())
				}
			}
		} else {
			log.Printf("syncServices: No services with changed ports found, skipping IEAgAgRule updates")
		}
	}

	if err := writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Process conditions for IEAGAG rules created during service sync (now with proper port aggregation)
	for i := range allNewIEAgAgRules {
		if err := s.conditionManager.ProcessIEAgAgRuleConditions(ctx, &allNewIEAgAgRules[i]); err != nil {
			log.Printf("Failed to process IEAgAgRule conditions for %s: %v", allNewIEAgAgRules[i].Key(), err)
		}
		if err := s.conditionManager.saveResourceConditions(ctx, &allNewIEAgAgRules[i]); err != nil {
			log.Printf("Failed to save IEAgAgRule conditions for %s: %v", allNewIEAgAgRules[i].Key(), err)
		}
	}

	return nil
}

// CreateRuleS2S создает новое правило s2s
func (s *NetguardService) CreateRuleS2S(ctx context.Context, rule models.RuleS2S) error {
	log.Printf("CreateRuleS2S: Starting creation of RuleS2S %s", rule.Key())

	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	ruleValidator := validator.GetRuleS2SValidator()

	// Валидируем правило перед созданием
	log.Printf("CreateRuleS2S: Validating RuleS2S %s", rule.Key())
	if err := ruleValidator.ValidateForCreation(ctx, rule); err != nil {
		log.Printf("CreateRuleS2S: Validation failed for RuleS2S %s: %v", rule.Key(), err)
		return err
	}
	log.Printf("CreateRuleS2S: Validation passed for RuleS2S %s", rule.Key())

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			log.Printf("CreateRuleS2S: Aborting transaction for RuleS2S %s due to error: %v", rule.Key(), err)
			writer.Abort()
		}
	}()

	// ИЗМЕНЕНИЕ: Используем syncRuleS2S для генерации IEAgAgRule и заполнения IEAgAgRuleRefs
	log.Printf("CreateRuleS2S: Syncing RuleS2S %s with IEAgAgRule generation", rule.Key())
	if err = s.syncRuleS2S(ctx, writer, []models.RuleS2S{rule}, models.SyncOpUpsert); err != nil {
		log.Printf("CreateRuleS2S: Failed to sync RuleS2S %s: %v", rule.Key(), err)
		return errors.Wrap(err, "failed to create rule s2s")
	}

	if err = writer.Commit(); err != nil {
		log.Printf("CreateRuleS2S: Failed to commit transaction for RuleS2S %s: %v", rule.Key(), err)
		return errors.Wrap(err, "failed to commit")
	}
	log.Printf("CreateRuleS2S: Successfully committed RuleS2S %s", rule.Key())

	// Обработка conditions
	log.Printf("CreateRuleS2S: Processing conditions for RuleS2S %s", rule.Key())
	s.conditionManager.ProcessRuleS2SConditions(ctx, &rule)
	if err := s.conditionManager.saveResourceConditions(ctx, &rule); err != nil {
		log.Printf("CreateRuleS2S: Failed to save conditions for RuleS2S %s: %v", rule.Key(), err)
		return errors.Wrap(err, "failed to save rule s2s conditions")
	}

	log.Printf("CreateRuleS2S: Successfully created RuleS2S %s", rule.Key())
	return nil
}

// UpdateRuleS2S обновляет существующее правило s2s
func (s *NetguardService) UpdateRuleS2S(ctx context.Context, rule models.RuleS2S) error {
	log.Printf("UpdateRuleS2S: Starting update of RuleS2S %s", rule.Key())

	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Получаем старую версию правила
	log.Printf("UpdateRuleS2S: Fetching existing RuleS2S %s", rule.Key())
	oldRule, err := reader.GetRuleS2SByID(ctx, rule.ResourceIdentifier)
	if err != nil {
		log.Printf("UpdateRuleS2S: Failed to get existing RuleS2S %s: %v", rule.Key(), err)
		return errors.Wrap(err, "failed to get existing rule s2s")
	}
	log.Printf("UpdateRuleS2S: Found existing RuleS2S %s with %d IEAgAgRuleRefs",
		oldRule.Key(), len(oldRule.IEAgAgRuleRefs))

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	ruleValidator := validator.GetRuleS2SValidator()

	// Валидируем правило перед обновлением
	log.Printf("UpdateRuleS2S: Validating update of RuleS2S %s", rule.Key())
	if err := ruleValidator.ValidateForUpdate(ctx, *oldRule, rule); err != nil {
		log.Printf("UpdateRuleS2S: Validation failed for RuleS2S %s: %v", rule.Key(), err)
		return err
	}
	log.Printf("UpdateRuleS2S: Validation passed for RuleS2S %s", rule.Key())

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			log.Printf("UpdateRuleS2S: Aborting transaction for RuleS2S %s due to error: %v", rule.Key(), err)
			writer.Abort()
		}
	}()

	// ИЗМЕНЕНИЕ: Используем syncRuleS2S для пересоздания IEAgAgRule и обновления IEAgAgRuleRefs
	log.Printf("UpdateRuleS2S: Syncing RuleS2S %s with IEAgAgRule regeneration", rule.Key())
	if err = s.syncRuleS2S(ctx, writer, []models.RuleS2S{rule}, models.SyncOpUpsert); err != nil {
		log.Printf("UpdateRuleS2S: Failed to sync RuleS2S %s: %v", rule.Key(), err)
		return errors.Wrap(err, "failed to update rule s2s")
	}

	if err = writer.Commit(); err != nil {
		log.Printf("UpdateRuleS2S: Failed to commit transaction for RuleS2S %s: %v", rule.Key(), err)
		return errors.Wrap(err, "failed to commit")
	}
	log.Printf("UpdateRuleS2S: Successfully committed RuleS2S %s", rule.Key())

	// Обработка conditions
	log.Printf("UpdateRuleS2S: Processing conditions for RuleS2S %s", rule.Key())
	s.conditionManager.ProcessRuleS2SConditions(ctx, &rule)
	if err := s.conditionManager.saveResourceConditions(ctx, &rule); err != nil {
		log.Printf("UpdateRuleS2S: Failed to save conditions for RuleS2S %s: %v", rule.Key(), err)
		return errors.Wrap(err, "failed to save rule s2s conditions")
	}

	log.Printf("UpdateRuleS2S: Successfully updated RuleS2S %s", rule.Key())
	return nil
}

// findRuleS2SForServices finds all RuleS2S that reference the given services
func (s *NetguardService) findRuleS2SForServices(ctx context.Context, serviceIDs []models.ResourceIdentifier) ([]models.RuleS2S, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	return s.findRuleS2SForServicesWithReader(ctx, reader, serviceIDs)
}

// findRuleS2SForServicesWithReader finds all RuleS2S that reference the given services using the provided reader
func (s *NetguardService) findRuleS2SForServicesWithReader(ctx context.Context, reader ports.Reader, serviceIDs []models.ResourceIdentifier) ([]models.RuleS2S, error) {
	// First, find all ServiceAliases that reference these services
	var serviceAliases []models.ServiceAlias
	err := reader.ListServiceAliases(ctx, func(alias models.ServiceAlias) error {
		for _, serviceID := range serviceIDs {
			if alias.ServiceRef.Key() == serviceID.Key() {
				serviceAliases = append(serviceAliases, alias)
				break
			}
		}
		return nil
	}, nil)

	if err != nil {
		return nil, errors.Wrap(err, "failed to list service aliases")
	}

	// Create a map of service alias IDs for quick lookup
	serviceAliasMap := make(map[string]bool)
	for _, alias := range serviceAliases {
		serviceAliasMap[alias.Key()] = true
	}

	// Now find all RuleS2S that reference these service aliases
	var rules []models.RuleS2S
	err = reader.ListRuleS2S(ctx, func(rule models.RuleS2S) error {
		// Check if the rule references any of the service aliases
		if serviceAliasMap[rule.ServiceLocalRef.Key()] || serviceAliasMap[rule.ServiceRef.Key()] {
			rules = append(rules, rule)
		}
		return nil
	}, nil)

	if err != nil {
		return nil, errors.Wrap(err, "failed to list rules")
	}

	return rules, nil
}

// // updateIEAgAgRulesForRuleS2S updates the IEAgAgRules for the given RuleS2S
// // syncOp - операция синхронизации (FullSync, Upsert, Delete)
// func (s *NetguardService) updateIEAgAgRulesForRuleS2S(ctx context.Context, writer ports.Writer, rules []models.RuleS2S, syncOp models.SyncOp) error {
// 	// Get all existing IEAgAgRules to detect obsolete ones
// 	reader, err := s.registry.Reader(ctx)
// 	if err != nil {
// 		return errors.Wrap(err, "failed to get reader")
// 	}
// 	defer reader.Close()

// 	return s.updateIEAgAgRulesForRuleS2SWithReader(ctx, writer, reader, rules, syncOp)
// }

// // updateIEAgAgRulesForRuleS2SWithReaderNoConditions updates the IEAgAgRules for the given RuleS2S using the provided reader without processing conditions
// // This version is used when conditions will be processed separately (e.g., in syncServices)
// // syncOp - операция синхронизации (FullSync, Upsert, Delete)
// func (s *NetguardService) updateIEAgAgRulesForRuleS2SWithReaderNoConditions(ctx context.Context, writer ports.Writer, reader ports.Reader, rules []models.RuleS2S, syncOp models.SyncOp) error {
// 	// Get all existing IEAgAgRules to detect obsolete ones
// 	existingRules := make(map[string]models.IEAgAgRule)
// 	err := reader.ListIEAgAgRules(ctx, func(rule models.IEAgAgRule) error {
// 		existingRules[rule.Key()] = rule
// 		return nil
// 	}, nil)

// 	if err != nil {
// 		return errors.Wrap(err, "failed to list existing IEAgAgRules")
// 	}

// 	// Create a map of expected rules after the update
// 	expectedRules := make(map[string]bool)
// 	var allNewRules []models.IEAgAgRule

// 	// Generate IEAgAgRules for each RuleS2S
// 	for _, rule := range rules {
// 		ieAgAgRules, err := s.GenerateIEAgAgRulesFromRuleS2SWithReader(ctx, reader, rule)
// 		if err != nil {
// 			return errors.Wrapf(err, "failed to generate IEAgAgRules for RuleS2S %s", rule.Key())
// 		}

// 		// Add generated rules to the expected rules map and collect all new rules
// 		for _, ieRule := range ieAgAgRules {
// 			expectedRules[ieRule.Key()] = true
// 			allNewRules = append(allNewRules, ieRule)
// 		}
// 	}

// 	// Sync all new rules at once WITH sgroups synchronization
// 	if len(allNewRules) > 0 {
// 		if err = s.syncIEAgAgRules(ctx, writer, allNewRules, syncOp); err != nil {
// 			return errors.Wrap(err, "failed to sync new IEAgAgRules")
// 		}
// 	}

// 	// Find and delete obsolete rules
// 	var obsoleteRules []models.IEAgAgRule
// 	for key, rule := range existingRules {
// 		if !expectedRules[key] {
// 			obsoleteRules = append(obsoleteRules, rule)
// 		}
// 	}

// 	if len(obsoleteRules) > 0 {
// 		klog.Infof("🗑️ NO_CONDITIONS: Deleting %d obsolete IEAgAgRules", len(obsoleteRules))

// 		// 📋 STEP 1: Log all obsolete rules that will be deleted
// 		for i, rule := range obsoleteRules {
// 			klog.Infof("  🗑️ OBSOLETE[%d]: %s (%s/%s %s→%s ports=%s)",
// 				i, rule.Key(), rule.Transport, rule.Traffic,
// 				rule.AddressGroupLocal.Key(), rule.AddressGroup.Key(),
// 				rule.Ports[0].Destination)
// 		}

// 		// 🚀 STEP 2: CRITICAL - Sync DELETE operation with sgroups FIRST!
// 		klog.Infof("🔄 NO_CONDITIONS: Syncing DELETE operation with sgroups for %d obsolete rules", len(obsoleteRules))
// 		if err = s.syncIEAgAgRules(ctx, writer, obsoleteRules, models.SyncOpDelete); err != nil {
// 			klog.Errorf("❌ NO_CONDITIONS: Failed to sync obsolete rules deletion with sgroups: %v", err)
// 			return errors.Wrap(err, "failed to sync obsolete IEAgAgRules deletion with sgroups")
// 		}
// 		klog.Infof("✅ NO_CONDITIONS: Successfully synced DELETE operation with sgroups")

// 		// 📋 STEP 3: Delete from database AFTER sgroups sync
// 		var obsoleteRuleIDs []models.ResourceIdentifier
// 		for _, rule := range obsoleteRules {
// 			obsoleteRuleIDs = append(obsoleteRuleIDs, rule.ResourceIdentifier)
// 		}

// 		klog.Infof("🗃️ NO_CONDITIONS: Deleting %d obsolete rules from database", len(obsoleteRuleIDs))
// 		if err = writer.DeleteIEAgAgRulesByIDs(ctx, obsoleteRuleIDs); err != nil {
// 			klog.Errorf("❌ NO_CONDITIONS: Failed to delete obsolete rules from database: %v", err)
// 			return errors.Wrap(err, "failed to delete obsolete IEAgAgRules from database")
// 		}
// 		klog.Infof("✅ NO_CONDITIONS: Successfully deleted %d obsolete rules from database", len(obsoleteRuleIDs))
// 	} else {
// 		klog.Infof("✅ NO_CONDITIONS: No obsolete rules to delete")
// 	}

// 	return nil
// }

// updateIEAgAgRulesForRuleS2SWithReader updates the IEAgAgRules using SERVICE-CENTRIC approach
// This new implementation aggregates rules by affected services to prevent duplicates
// syncOp - операция синхронизации (FullSync, Upsert, Delete)
func (s *NetguardService) updateIEAgAgRulesForRuleS2SWithReader(ctx context.Context, writer ports.Writer, reader ports.Reader, rules []models.RuleS2S, syncOp models.SyncOp) error {
	klog.Infof("🚀 SERVICE_CENTRIC: updateIEAgAgRulesForRuleS2SWithReader called with %d RuleS2S, syncOp=%s", len(rules), syncOp)
	
	// STEP 1: Identify all affected services from the input RuleS2S
	affectedServices := make(map[string]models.ResourceIdentifier)
	
	for i, rule := range rules {
		klog.Infof("🔍 SERVICE_CENTRIC: Processing RuleS2S[%d]: %s (traffic=%s, local=%s, target=%s)",
			i, rule.Key(), rule.Traffic, rule.ServiceLocalRef.Key(), rule.ServiceRef.Key())
		
		// Get services referenced by this rule
		localService, err := reader.GetServiceByAlias(ctx, rule.ServiceLocalRef.ResourceIdentifier)
		if err != nil {
			klog.Errorf("❌ SERVICE_CENTRIC: Failed to get local service %s: %v", rule.ServiceLocalRef.Key(), err)
			continue
		}
		
		targetService, err := reader.GetServiceByAlias(ctx, rule.ServiceRef.ResourceIdentifier)
		if err != nil {
			klog.Errorf("❌ SERVICE_CENTRIC: Failed to get target service %s: %v", rule.ServiceRef.Key(), err)
			continue
		}
		
		// Add both services to affected services map
		affectedServices[localService.Key()] = localService.ResourceIdentifier
		affectedServices[targetService.Key()] = targetService.ResourceIdentifier
		
		klog.Infof("  ✅ SERVICE_CENTRIC: Added affected services - local: %s, target: %s",
			localService.Key(), targetService.Key())
	}
	
	klog.Infof("🎯 SERVICE_CENTRIC: Found %d unique affected services", len(affectedServices))
	
	// STEP 2: For each affected service, regenerate ALL its related IEAgAg rules
	return s.updateIEAgAgRulesForAffectedServices(ctx, writer, reader, affectedServices, syncOp)
}

// updateIEAgAgRulesForAffectedServices regenerates IEAgAg rules for all affected services
func (s *NetguardService) updateIEAgAgRulesForAffectedServices(ctx context.Context, writer ports.Writer, reader ports.Reader, affectedServices map[string]models.ResourceIdentifier, syncOp models.SyncOp) error {
	klog.Infof("🔄 SERVICE_REBUILD: Processing %d affected services", len(affectedServices))
	
	// TEST: Use new ListAllRelatedRuleS2S function
	for serviceKey, serviceID := range affectedServices {
		klog.Infof("🔍 SERVICE_REBUILD: Testing ListAllRelatedRuleS2S for service %s", serviceKey)
		
		relatedRules, err := s.backendClient.ListAllRelatedRuleS2S(ctx, serviceID)
		if err != nil {
			klog.Errorf("❌ SERVICE_REBUILD: Failed to find related rules for service %s: %v", serviceKey, err)
			continue
		}
		
		klog.Infof("🎯 SERVICE_REBUILD: Found %d related RuleS2S for service %s", len(relatedRules), serviceKey)
		for _, rule := range relatedRules {
			klog.Infof("  📋 SERVICE_REBUILD: Related rule: %s", rule.Key())
		}
	}
	
	// TODO: Complete implementation later
	klog.Infof("⚠️  SERVICE_REBUILD: Full implementation in progress...")
	return nil
}



// TODO: Implement helper functions for service-centric approach

// SyncServices syncs services
func (s *NetguardService) SyncServices(ctx context.Context, services []models.Service, scope ports.Scope) error {
	// TODO: Implement proper service synchronization
	klog.Infof("SyncServices called with %d services", len(services))
	return nil
}

// syncAddressGroups синхронизирует группы адресов с указанной операцией
func (s *NetguardService) syncAddressGroups(ctx context.Context, writer ports.Writer, addressGroups []models.AddressGroup, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

					klog.Infof("🔍 FULL_AGGREGATION: Processing AG group[%d,%d] %s → %s (%s %s)",
						i, j, localAG.Key(), targetAG.Key(), rule.Traffic, protocol)

					// 🔥 CRITICAL: Check if the relevant service has ports for this traffic direction
					var relevantService models.Service
					var relevantServiceName string
					if rule.Traffic == models.INGRESS {
						relevantService = *localService
						relevantServiceName = "LOCAL"
					} else {
						relevantService = *targetService
						relevantServiceName = "TARGET"
					}

					// Filter ports by protocol for the relevant service
					var relevantPorts []models.IngressPort
					for _, port := range relevantService.IngressPorts {
						if port.Protocol == protocol {
							relevantPorts = append(relevantPorts, port)
						}
					}

					klog.Infof("🔍 RELEVANCE_CHECK: %s traffic uses %s service %s with %d %s ports",
						rule.Traffic, relevantServiceName, relevantService.Key(), len(relevantPorts), protocol)

					// Skip if the relevant service has no ports for this protocol
					if len(relevantPorts) == 0 {
						klog.Infof("⚠️ AGGREGATION: Skipping AG group - %s service %s has no %s ports",
							relevantServiceName, relevantService.Key(), protocol)
						continue
					}

					// ✅ CRITICAL: Find ALL contributing RuleS2S for this AG pair!
					contributingRules, err := s.findContributingRuleS2S(ctx, reader, rule, *localService, *targetService)
					if err != nil {
						klog.Errorf("🚨 AGGREGATION: Failed to find contributing rules: %v", err)
						continue
					}

					// ✅ CRITICAL: Aggregate ports from ALL contributing rules for this protocol!
					aggregatedPorts := s.aggregatePortsWithProtocol(contributingRules, protocol)

					klog.Infof("🎯 FULL_AGGREGATION: AG pair %s → %s (%s %s) has %d contributing rules → %d aggregated ports: %s",
						localAG.Key(), targetAG.Key(), rule.Traffic, protocol,
						len(contributingRules), len(aggregatedPorts), strings.Join(aggregatedPorts, ","))

					// Create the port group only if we have aggregated ports
					if len(aggregatedPorts) > 0 {
						if portGroups[groupKey] == nil {
							portGroups[groupKey] = make(map[string]bool)
							ruleMetadata[groupKey] = ruleGroupMetadata{
								traffic:   rule.Traffic,
								localAG:   localAG,
								targetAG:  targetAG,
								protocol:  protocol,
								namespace: determineRuleNamespace(rule, localAG, targetAG),
							}
						}

						// Add all aggregated ports to the group
						for _, port := range aggregatedPorts {
							portGroups[groupKey][port] = true
						}

						klog.Infof("✅ FULL_AGGREGATION: Added %d aggregated ports to group %s",
							len(aggregatedPorts), groupKey)
					} else {
						klog.Infof("🗑️ FULL_AGGREGATION: Skipping group %s with 0 aggregated ports", groupKey)
					}
				}
			}
		}

	}

	// STEP 3: Create IEAgAgRules with AGGREGATED ports (TCP and UDP separately!)
	expectedRules := make(map[string]bool)
	var allNewRules []models.IEAgAgRule

	klog.Infof("🔄 AGGREGATION: Creating IEAgAgRules from %d port groups", len(portGroups))
	klog.Infof("📊 SUMMARY: Total port groups by protocol:")
	tcpGroupCount := 0
	udpGroupCount := 0
	for groupKey := range portGroups {
		if strings.Contains(groupKey, "|TCP") {
			tcpGroupCount++
		} else if strings.Contains(groupKey, "|UDP") {
			udpGroupCount++
		}
	}
	klog.Infof("  📊 TCP groups: %d, UDP groups: %d", tcpGroupCount, udpGroupCount)

	for groupKey, portsSet := range portGroups {
		metadata := ruleMetadata[groupKey]
		klog.Infof("🔨 RULE_CREATE: Processing group %s", groupKey)

		// Collect ports into slice and sort for determinism
		ports := make([]string, 0, len(portsSet))
		for port := range portsSet {
			ports = append(ports, port)
		}
		klog.Infof("  🔨 Collected %d ports from set: %v", len(ports), ports)

		// Skip rules with no ports (they will be cleaned up as obsolete)
		if len(ports) == 0 {
			klog.Infof("🗑️ AGGREGATION: Skipping group %s with 0 ports (will be cleaned up)", groupKey)
			continue
		}

		// Create rule with AGGREGATED ports
		ruleName := generateRuleName(string(metadata.traffic), metadata.localAG.Name, metadata.targetAG.Name, string(metadata.protocol))
		klog.Infof("  🔨 Rule name generated: %s", ruleName)

		ieRule := models.IEAgAgRule{
			SelfRef: models.SelfRef{
				ResourceIdentifier: models.NewResourceIdentifier(
					ruleName,
					models.WithNamespace(metadata.namespace),
				),
			},
			Transport:         metadata.protocol,
			Traffic:           metadata.traffic,
			AddressGroupLocal: metadata.localAG,
			AddressGroup:      metadata.targetAG,
			Ports: []models.PortSpec{
				{
					Destination: strings.Join(ports, ","), // ALL ports from ALL RuleS2S combined!
				},
			},
			Action:   models.ActionAccept,
			Logs:     false,
			Trace:    false,
			Priority: 0,
		}

		klog.Infof("  🔨 RULE_FIELDS:")
		klog.Infof("    📍 Key: %s", ieRule.Key())
		klog.Infof("    📍 Name: %s", ieRule.SelfRef.Name)
		klog.Infof("    📍 Namespace: %s", ieRule.SelfRef.Namespace)
		klog.Infof("    📍 Transport: %s", ieRule.Transport)
		klog.Infof("    📍 Traffic: %s", ieRule.Traffic)
		klog.Infof("    📍 LocalAG: %s", ieRule.AddressGroupLocal.Key())
		klog.Infof("    📍 TargetAG: %s", ieRule.AddressGroup.Key())
		klog.Infof("    📍 Ports: %s", ieRule.Ports[0].Destination)
		klog.Infof("    📍 Action: %s", ieRule.Action)

		expectedRules[ieRule.Key()] = true
		allNewRules = append(allNewRules, ieRule)

		klog.Infof("✅ AGGREGATION: Created %s IEAgAgRule %s with %d aggregated ports: %s",
			metadata.protocol, ieRule.Key(), len(ports), strings.Join(ports, ","))
	}

	// STEP 4: Sync all new rules at once
	if len(allNewRules) > 0 {
		klog.Infof("📝 AGGREGATION: Syncing %d new IEAgAgRules", len(allNewRules))
		if err = s.syncIEAgAgRules(ctx, writer, allNewRules, syncOp); err != nil {
			return errors.Wrap(err, "failed to sync new IEAgAgRules")
		}
	}

	// STEP 5: Find and delete obsolete rules
	var obsoleteRules []models.IEAgAgRule
	for key, rule := range existingRules {
		if !expectedRules[key] {
			obsoleteRules = append(obsoleteRules, rule)
			klog.Infof("🗑️ AGGREGATION: Marking rule %s as obsolete", key)
		}
	}

	klog.Infof("🔥 AGGREGATION: ================= OBSOLETE DELETION ANALYSIS =================")
	klog.Infof("🔥 AGGREGATION: Total existing rules: %d", len(existingRules))
	klog.Infof("🔥 AGGREGATION: Total expected rules: %d", len(expectedRules))
	klog.Infof("🔥 AGGREGATION: Calculating obsolete rules...")

	if len(obsoleteRules) > 0 {
		klog.Infof("🗑️ AGGREGATION: Deleting %d obsolete IEAgAgRules", len(obsoleteRules))

		// 📋 STEP 1: Log all obsolete rules that will be deleted
		for i, rule := range obsoleteRules {
			klog.Infof("  🗑️ OBSOLETE[%d]: %s (%s/%s %s→%s ports=%s)",
				i, rule.Key(), rule.Transport, rule.Traffic,
				rule.AddressGroupLocal.Key(), rule.AddressGroup.Key(),
				rule.Ports[0].Destination)
		}

		// 🚀 STEP 2: CRITICAL - Sync DELETE operation with sgroups FIRST!
		klog.Infof("🔄 AGGREGATION: Syncing DELETE operation with sgroups for %d obsolete rules", len(obsoleteRules))
		if err = s.syncIEAgAgRules(ctx, writer, obsoleteRules, models.SyncOpDelete); err != nil {
			klog.Errorf("❌ AGGREGATION: Failed to sync obsolete rules deletion with sgroups: %v", err)
			return errors.Wrap(err, "failed to sync obsolete IEAgAgRules deletion with sgroups")
		}
		klog.Infof("✅ AGGREGATION: Successfully synced DELETE operation with sgroups")

		// 📋 STEP 3: Delete from database AFTER sgroups sync
		var obsoleteRuleIDs []models.ResourceIdentifier
		for _, rule := range obsoleteRules {
			obsoleteRuleIDs = append(obsoleteRuleIDs, rule.ResourceIdentifier)
		}

		klog.Infof("🗃️ AGGREGATION: Deleting %d obsolete rules from database", len(obsoleteRuleIDs))
		if err = writer.DeleteIEAgAgRulesByIDs(ctx, obsoleteRuleIDs); err != nil {
			klog.Errorf("❌ AGGREGATION: Failed to delete obsolete rules from database: %v", err)
			return errors.Wrap(err, "failed to delete obsolete IEAgAgRules from database")
		}
		klog.Infof("✅ AGGREGATION: Successfully deleted %d obsolete rules from database", len(obsoleteRuleIDs))
	} else {
		klog.Infof("✅ AGGREGATION: No obsolete rules to delete")
	}

	klog.Infof("✅ AGGREGATION: updateIEAgAgRulesForRuleS2SWithReader completed successfully")

	// 📊 FINAL SUMMARY
	klog.Infof("📊 AGGREGATION SUMMARY:")
	klog.Infof("    📊 Processed %d RuleS2S", len(rules))
	klog.Infof("    📊 Found %d existing IEAgAgRules", len(existingRules))
	klog.Infof("    📊 Created %d new/updated IEAgAgRules", len(allNewRules))
	klog.Infof("    📊 Deleted %d obsolete IEAgAgRules", len(obsoleteRules))
	klog.Infof("    📊 Net change: %+d rules (%d existing → %d expected)",
		len(allNewRules)-len(obsoleteRules), len(existingRules), len(expectedRules))

	return nil
}

// SyncServices syncs services
func (s *NetguardService) SyncServices(ctx context.Context, services []models.Service, scope ports.Scope) error {
	klog.Infof("🔥 DEBUG: PUBLIC SyncServices called with %d services", len(services))
	for i, service := range services {
		klog.Infof("🔥 DEBUG: PUBLIC Service[%d]: %s, ports=%d", i, service.Key(), len(service.IngressPorts))
		for j, port := range service.IngressPorts {
			klog.Infof("🔥 DEBUG: PUBLIC Service[%d].IngressPorts[%d]: %s/%s", i, j, port.Port, port.Protocol)
		}
	}
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	serviceValidator := validator.GetServiceValidator()

	// Validate all services
	for _, service := range services {
		// Check if service exists
		existingService, err := reader.GetServiceByID(ctx, service.ResourceIdentifier)
		if err == nil {
			// Service exists - use ValidateForUpdate
			if err := serviceValidator.ValidateForUpdate(ctx, *existingService, service); err != nil {
				return err
			}
		} else {
			// Service is new - use ValidateForCreation
			if err := serviceValidator.ValidateForCreation(ctx, service); err != nil {
				return err
			}
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncServices(ctx, services, scope, ports.WithSyncOp(models.SyncOpFullSync)); err != nil {
		return errors.Wrap(err, "failed to sync services")
	}

	// After successfully syncing services, update related IEAgAgRules
	// OPTIMIZATION: Only update IEAgAgRules for services with changed ports
	var servicesWithChangedPorts []models.ResourceIdentifier
	for _, service := range services {
		// Get existing service for comparison
		existingService, err := reader.GetServiceByID(ctx, service.ResourceIdentifier)
		if err != nil {
			// New service - add to update list
			log.Printf("Service %s is new, will update related IEAgAgRules", service.Key())
			servicesWithChangedPorts = append(servicesWithChangedPorts, service.ResourceIdentifier)
			continue
		}

		// Compare ports between old and new version
		if s.servicePortsChanged(*existingService, service) {
			servicesWithChangedPorts = append(servicesWithChangedPorts, service.ResourceIdentifier)
			log.Printf("Service %s has changed ports, will update related IEAgAgRules", service.Key())
		} else {
			log.Printf("Service %s ports unchanged, skipping IEAgAgRule update", service.Key())
		}
	}

	// Update IEAgAgRules only for services with changed ports
	var allNewIEAgAgRules []models.IEAgAgRule
	if len(servicesWithChangedPorts) > 0 {
		log.Printf("Found %d services with changed ports, updating related IEAgAgRules", len(servicesWithChangedPorts))

		// Find all RuleS2S that reference services with changed ports
		affectedRules, err := s.findRuleS2SForServices(ctx, servicesWithChangedPorts)
		if err != nil {
			writer.Abort()
			return errors.Wrap(err, "failed to find affected RuleS2S")
		}

		log.Printf("Found %d RuleS2S affected by port changes", len(affectedRules))

		if len(affectedRules) > 0 {
			// Get reader that can see changes in current transaction
			txReader, err := s.registry.ReaderFromWriter(ctx, writer)
			if err != nil {
				writer.Abort()
				return errors.Wrap(err, "failed to get transaction reader")
			}
			defer txReader.Close()

			// Update IEAgAgRules with proper port aggregation logic
			if err = s.updateIEAgAgRulesForRuleS2SWithReader(ctx, writer, txReader, affectedRules, models.SyncOpFullSync); err != nil {
				writer.Abort()
				return errors.Wrap(err, "failed to update IEAgAgRules for affected RuleS2S")
			}

			// Get all IEAgAgRules from DB for conditions processing (after aggregation)
			txReader2, err := s.registry.ReaderFromWriter(ctx, writer)
			if err != nil {
				writer.Abort()
				return errors.Wrap(err, "failed to get transaction reader for conditions")
			}
			defer txReader2.Close()

			err = txReader2.ListIEAgAgRules(ctx, func(ieRule models.IEAgAgRule) error {
				allNewIEAgAgRules = append(allNewIEAgAgRules, ieRule)
				return nil
			}, nil)
			if err != nil {
				writer.Abort()
				return errors.Wrap(err, "failed to list all IEAgAgRules for conditions processing")
			}
			log.Printf("SyncServices: Found %d IEAgAgRules for conditions processing", len(allNewIEAgAgRules))
		}
	} else {
		log.Printf("No services with changed ports found, skipping IEAgAgRule updates")
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Обработка conditions после успешного commit
	for i := range services {
		s.conditionManager.ProcessServiceConditions(ctx, &services[i])
		if err := s.conditionManager.saveResourceConditions(ctx, &services[i]); err != nil {
			log.Printf("Failed to save service conditions for %s: %v", services[i].Key(), err)
		}
	}

	// Process conditions for IEAGAG rules created during service update
	for i := range allNewIEAgAgRules {
		if err := s.conditionManager.ProcessIEAgAgRuleConditions(ctx, &allNewIEAgAgRules[i]); err != nil {
			log.Printf("Failed to process IEAgAgRule conditions for %s: %v", allNewIEAgAgRules[i].Key(), err)
		}
		if err := s.conditionManager.saveResourceConditions(ctx, &allNewIEAgAgRules[i]); err != nil {
			log.Printf("Failed to save IEAgAgRule conditions for %s: %v", allNewIEAgAgRules[i].Key(), err)
		}
	}
	return nil
}

// syncAddressGroups синхронизирует группы адресов с указанной операцией
func (s *NetguardService) syncAddressGroups(ctx context.Context, writer ports.Writer, addressGroups []models.AddressGroup, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

		validator := validation.NewDependencyValidator(reader)
		addressGroupValidator := validator.GetAddressGroupValidator()

		for _, addressGroup := range addressGroups {
			existingAddressGroup, err := reader.GetAddressGroupByID(ctx, addressGroup.ResourceIdentifier)
			if err == nil && syncOp != models.SyncOpDelete {
				// Группа адресов существует - используем ValidateForUpdate
				if err := addressGroupValidator.ValidateForUpdate(ctx, *existingAddressGroup, addressGroup); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound && syncOp != models.SyncOpDelete {
				// Группа адресов новая - используем ValidateForCreation
				if err := addressGroupValidator.ValidateForCreation(ctx, addressGroup); err != nil {
					return err
				}
			} else if err != nil && err != ports.ErrNotFound {
				// Произошла другая ошибка
				return errors.Wrap(err, "failed to get address group")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if syncOp == models.SyncOpFullSync {
		// При операции FullSync используем пустую область видимости,
		// чтобы удалить все группы адресов, а затем добавить только новые
		scope = ports.EmptyScope{}
	} else if len(addressGroups) > 0 {
		var ids []models.ResourceIdentifier
		for _, addressGroup := range addressGroups {
			ids = append(ids, addressGroup.ResourceIdentifier)
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Если это удаление, используем DeleteAddressGroupsByIDs для корректного каскадного удаления
	if syncOp == models.SyncOpDelete {
		// Собираем ID групп адресов
		var ids []models.ResourceIdentifier
		for _, addressGroup := range addressGroups {
			ids = append(ids, addressGroup.ResourceIdentifier)
		}

		// Используем DeleteAddressGroupsByIDs для каскадного удаления групп адресов и связанных ресурсов
		return s.DeleteAddressGroupsByIDs(ctx, ids)
	}

	// Выполнение операции с указанной опцией для не-удаления
	if err := writer.SyncAddressGroups(ctx, addressGroups, scope, ports.WithSyncOp(syncOp)); err != nil {
		log.Printf("❌ ERROR: syncAddressGroups - Failed to sync address groups to writer: %v", err)
		return errors.Wrap(err, "failed to sync address groups")
	}

	if err := writer.Commit(); err != nil {
		log.Printf("❌ ERROR: syncAddressGroups - Failed to commit transaction: %v", err)
		return errors.Wrap(err, "failed to commit")
	}

	// Синхронизация с sgroups после успешного commit'а (только для операций создания/обновления)
	if syncOp != models.SyncOpDelete {
		s.syncAddressGroupsWithSGroups(ctx, addressGroups, types.SyncOperationUpsert)
	}

	return nil
}

// findServicesForAddressGroups finds all Services that reference the given address groups
func (s *NetguardService) findServicesForAddressGroups(ctx context.Context, addressGroupIDs []models.ResourceIdentifier) ([]models.Service, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create a map of address group IDs for quick lookup
	addressGroupMap := make(map[string]bool)
	for _, id := range addressGroupIDs {
		addressGroupMap[id.Key()] = true
	}

	// Find all Services that reference these address groups
	var services []models.Service
	err = reader.ListServices(ctx, func(service models.Service) error {
		// Check if the service references any of the address groups
		for _, ag := range service.AddressGroups {
			if addressGroupMap[ag.Key()] {
				services = append(services, service)
				break
			}
		}
		return nil
	}, nil)

	if err != nil {
		return nil, errors.Wrap(err, "failed to list services")
	}

	return services, nil
}

// SyncAddressGroups syncs address groups
func (s *NetguardService) SyncAddressGroups(ctx context.Context, addressGroups []models.AddressGroup, scope ports.Scope) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	addressGroupValidator := validator.GetAddressGroupValidator()

	// Validate all address groups
	for _, addressGroup := range addressGroups {
		// Check if address group exists
		existingAddressGroup, err := reader.GetAddressGroupByID(ctx, addressGroup.ResourceIdentifier)
		if err == nil {
			// Address group exists - use ValidateForUpdate
			if err := addressGroupValidator.ValidateForUpdate(ctx, *existingAddressGroup, addressGroup); err != nil {
				return err
			}
		} else if err == ports.ErrNotFound {
			// Address group is new - use ValidateForCreation
			if err := addressGroupValidator.ValidateForCreation(ctx, addressGroup); err != nil {
				return err
			}
		} else {
			// Other error occurred
			return errors.Wrap(err, "failed to get address group")
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroups(ctx, addressGroups, scope, ports.WithSyncOp(models.SyncOpFullSync)); err != nil {
		return errors.Wrap(err, "failed to sync address groups")
	}

	// OPTIMIZATION: AddressGroup contains only IP addresses, not ports. IEAgAgRule updates are not needed here.
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Синхронизация с sgroups после успешного commit'а
	s.syncAddressGroupsWithSGroups(ctx, addressGroups, types.SyncOperationUpsert)

	return nil
}

// syncAddressGroupsWithSGroups синхронизирует AddressGroup с sgroups
func (s *NetguardService) syncAddressGroupsWithSGroups(ctx context.Context, addressGroups []models.AddressGroup, operation types.SyncOperation) {
	if s.syncManager == nil {
		return
	}

	for _, addressGroup := range addressGroups {
		if syncErr := s.syncManager.SyncEntity(ctx, &addressGroup, operation); syncErr != nil {
			log.Printf("❌ ERROR: syncAddressGroupsWithSGroups - Failed to sync AddressGroup %s with sgroups: %v", addressGroup.GetSyncKey(), syncErr)
			// Не прерываем обработку остальных AddressGroup - синхронизация может быть повторена позже
		}
	}
}

// syncAddressGroupsWithSGroupsForced синхронизирует AddressGroup с sgroups без debouncing
func (s *NetguardService) syncAddressGroupsWithSGroupsForced(ctx context.Context, addressGroups []models.AddressGroup, operation types.SyncOperation) {
	if s.syncManager == nil {
		return
	}

	for _, addressGroup := range addressGroups {
		if syncErr := s.syncManager.SyncEntityForced(ctx, &addressGroup, operation); syncErr != nil {
			log.Printf("❌ ERROR: syncAddressGroupsWithSGroupsForced - Failed to sync AddressGroup %s with sgroups: %v", addressGroup.GetSyncKey(), syncErr)
			// Не прерываем обработку остальных AddressGroup - синхронизация может быть повторена позже
		}
	}
}

// syncNetworksWithSGroups синхронизирует Network с sgroups
func (s *NetguardService) syncNetworksWithSGroups(ctx context.Context, networks []models.Network, operation types.SyncOperation) map[string]error {
	syncResults := make(map[string]error)

	if s.syncManager == nil {
		log.Printf("⚠️  WARNING: syncNetworksWithSGroups - SyncManager is nil, skipping sync for %d Networks", len(networks))
		// Возвращаем ошибки для всех Networks
		for _, network := range networks {
			syncResults[network.GetSyncKey()] = fmt.Errorf("SyncManager is nil")
		}
		return syncResults
	}

	log.Printf("🔧 DEBUG: syncNetworksWithSGroups - Starting sync process for %d Networks (operation: %s)", len(networks), operation)

	for _, network := range networks {
		log.Printf("🔧 DEBUG: syncNetworksWithSGroups - Attempting to sync Network %s with sgroups", network.GetSyncKey())
		log.Printf("🔧 DEBUG: syncNetworksWithSGroups - Network details: Name=%s, Namespace=%s, SyncSubjectType=%s",
			network.Name, network.Namespace, network.GetSyncSubjectType())

		if syncErr := s.syncManager.SyncEntity(ctx, &network, operation); syncErr != nil {
			log.Printf("❌ ERROR: syncNetworksWithSGroups - Failed to sync Network %s with sgroups: %v", network.GetSyncKey(), syncErr)
			syncResults[network.GetSyncKey()] = syncErr
		} else {
			log.Printf("✅ DEBUG: syncNetworksWithSGroups - Successfully initiated sync for Network %s", network.GetSyncKey())
			syncResults[network.GetSyncKey()] = nil // Успешная синхронизация
		}
	}

	log.Printf("✅ DEBUG: syncNetworksWithSGroups - Completed sync process for %d Networks", len(networks))
	return syncResults
}

// syncNetworksWithSGroupsForced синхронизирует Network с sgroups без debouncing
func (s *NetguardService) syncNetworksWithSGroupsForced(ctx context.Context, networks []models.Network, operation types.SyncOperation) map[string]error {
	syncResults := make(map[string]error)

	if s.syncManager == nil {
		log.Printf("⚠️  WARNING: syncNetworksWithSGroupsForced - SyncManager is nil, skipping sync for %d Networks", len(networks))
		// Возвращаем ошибки для всех Networks
		for _, network := range networks {
			syncResults[network.GetSyncKey()] = fmt.Errorf("SyncManager is nil")
		}
		return syncResults
	}

	log.Printf("🔧 DEBUG: syncNetworksWithSGroupsForced - Starting FORCED sync process for %d Networks (operation: %s)", len(networks), operation)

	for _, network := range networks {
		log.Printf("🔧 DEBUG: syncNetworksWithSGroupsForced - Attempting to FORCE sync Network %s with sgroups", network.GetSyncKey())
		log.Printf("🔧 DEBUG: syncNetworksWithSGroupsForced - Network details: Name=%s, Namespace=%s, SyncSubjectType=%s",
			network.Name, network.Namespace, network.GetSyncSubjectType())

		if syncErr := s.syncManager.SyncEntityForced(ctx, &network, operation); syncErr != nil {
			log.Printf("❌ ERROR: syncNetworksWithSGroupsForced - Failed to sync Network %s with sgroups: %v", network.GetSyncKey(), syncErr)
			syncResults[network.GetSyncKey()] = syncErr
		} else {
			log.Printf("✅ DEBUG: syncNetworksWithSGroupsForced - Successfully initiated FORCED sync for Network %s", network.GetSyncKey())
			syncResults[network.GetSyncKey()] = nil // Успешная синхронизация
		}
	}

	log.Printf("✅ DEBUG: syncNetworksWithSGroupsForced - Completed FORCED sync process for %d Networks", len(networks))
	return syncResults
}

// updateServiceAddressGroups updates Service.AddressGroups slice according to the operation ("add" or "remove").
// It returns true if the Service has been modified.
func (s *NetguardService) updateServiceAddressGroups(service *models.Service, addressGroupRef models.AddressGroupRef, operation string) bool {
	updated := false

	switch operation {
	case "add":
		found := false
		for _, ag := range service.AddressGroups {
			if ag.Key() == addressGroupRef.Key() {
				found = true
				break
			}
		}
		if !found {
			service.AddressGroups = append(service.AddressGroups, addressGroupRef)
			updated = true
		}
	case "remove":
		for i, ag := range service.AddressGroups {
			if ag.Key() == addressGroupRef.Key() {
				service.AddressGroups = append(service.AddressGroups[:i], service.AddressGroups[i+1:]...)
				updated = true
				break
			}
		}
	}

	return updated
}

// syncAddressGroupBindings синхронизирует привязки групп адресов с указанной операцией
func (s *NetguardService) syncAddressGroupBindings(ctx context.Context, writer ports.Writer, bindings []models.AddressGroupBinding, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

		validator := validation.NewDependencyValidator(reader)
		bindingValidator := validator.GetAddressGroupBindingValidator()

		for i := range bindings {
			// Use pointer to binding so we can modify it
			binding := &bindings[i]

			existingBinding, err := reader.GetAddressGroupBindingByID(ctx, binding.ResourceIdentifier)
			if err == nil && syncOp != models.SyncOpDelete {
				// Привязка существует - используем ValidateForUpdate
				if err := bindingValidator.ValidateForUpdate(ctx, *existingBinding, binding); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound && syncOp != models.SyncOpDelete {
				// Привязка новая - используем ValidateForCreation
				if err := bindingValidator.ValidateForCreation(ctx, binding); err != nil {
					return err
				}
			} else if err != nil && err != ports.ErrNotFound {
				// Произошла другая ошибка
				return errors.Wrap(err, "failed to get address group binding")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if syncOp == models.SyncOpFullSync {
		// При операции FullSync используем пустую область видимости,
		// чтобы удалить все привязки групп адресов, а затем добавить только новые
		scope = ports.EmptyScope{}
	} else if len(bindings) > 0 {
		var ids []models.ResourceIdentifier
		for _, binding := range bindings {
			ids = append(ids, binding.ResourceIdentifier)
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Если это удаление, используем DeleteAddressGroupBindingsByIDs для корректного каскадного удаления
	if syncOp == models.SyncOpDelete {
		// Собираем ID привязок групп адресов
		var ids []models.ResourceIdentifier
		for _, binding := range bindings {
			ids = append(ids, binding.ResourceIdentifier)
		}

		// Используем DeleteAddressGroupBindingsByIDs для каскадного удаления привязок и связанных ресурсов
		return s.DeleteAddressGroupBindingsByIDs(ctx, ids)
	}

	// Выполнение операции с указанной опцией для не-удаления
	if err := writer.SyncAddressGroupBindings(ctx, bindings, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync address group bindings")
	}

	// Синхронизируем port mappings для каждого binding, если это не удаление
	if syncOp != models.SyncOpDelete {
		for _, binding := range bindings {
			// Игнорируем ошибки при синхронизации port mappings, чтобы не блокировать основную операцию
			_ = s.SyncAddressGroupPortMappingsWithSyncOp(ctx, binding, syncOp)
		}
	}

	if err := writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	for i := range bindings {
		s.conditionManager.ProcessAddressGroupBindingConditions(ctx, &bindings[i])
		if err := s.conditionManager.saveResourceConditions(ctx, &bindings[i]); err != nil {
			log.Printf("Failed to save address group binding conditions for %s: %v", bindings[i].Key(), err)
		}
	}

	return nil
}

// SyncAddressGroupPortMappingsWithWriter обеспечивает синхронизацию port mapping для binding
// writer - существующий открытый writer для транзакции
// syncOp - операция синхронизации (FullSync, Upsert, Delete)
func (s *NetguardService) SyncAddressGroupPortMappingsWithWriter(ctx context.Context, writer ports.Writer, binding models.AddressGroupBinding, syncOp models.SyncOp) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	return s.SyncAddressGroupPortMappingsWithWriterAndReader(ctx, writer, reader, binding, syncOp)
}

// SyncAddressGroupPortMappingsWithWriterAndReader обеспечивает синхронизацию port mapping для binding
// writer - существующий открытый writer для транзакции
// reader - существующий открытый reader, который может видеть изменения в текущей транзакции
// syncOp - операция синхронизации (FullSync, Upsert, Delete)
func (s *NetguardService) SyncAddressGroupPortMappingsWithWriterAndReader(ctx context.Context, writer ports.Writer, reader ports.Reader, binding models.AddressGroupBinding, syncOp models.SyncOp) error {
	// Получаем сервис для доступа к его портам
	service, err := reader.GetServiceByID(ctx, binding.ServiceRef.ResourceIdentifier)
	if err == ports.ErrNotFound {
		return errors.New("service not found for port mapping")
	} else if err != nil {
		return errors.Wrapf(err, "failed to get service for port mapping")
	}

	// Проверяем существующий port mapping для этой address group
	portMapping, err := reader.GetAddressGroupPortMappingByID(ctx, binding.AddressGroupRef.ResourceIdentifier)

	var updatedMapping models.AddressGroupPortMapping

	if err == ports.ErrNotFound {
		// Port mapping не существует - создаем новый
		updatedMapping = *validation.CreateNewPortMapping(binding.AddressGroupRef.ResourceIdentifier, *service)
	} else if err != nil {
		// Произошла другая ошибка
		return errors.Wrap(err, "failed to get address group port mapping")
	} else {
		// Port mapping существует - обновляем его
		updatedMapping = *validation.UpdatePortMapping(*portMapping, binding.ServiceRef, *service)

		// Проверяем перекрытие портов
		if err := validation.CheckPortOverlaps(*service, updatedMapping); err != nil {
			return err
		}
	}

	// Используем переданный writer вместо создания нового
	if err = writer.SyncAddressGroupPortMappings(
		ctx,
		[]models.AddressGroupPortMapping{updatedMapping},
		ports.NewResourceIdentifierScope(updatedMapping.ResourceIdentifier),
		ports.WithSyncOp(syncOp),
	); err != nil {
		return errors.Wrap(err, "failed to sync address group port mappings")
	}

	return nil
}

// SyncAddressGroupPortMappings обеспечивает синхронизацию port mapping для binding
// с созданием собственной транзакции, используя операцию Upsert
func (s *NetguardService) SyncAddressGroupPortMappings(ctx context.Context, binding models.AddressGroupBinding) error {
	return s.SyncAddressGroupPortMappingsWithSyncOp(ctx, binding, models.SyncOpUpsert)
}

// SyncAddressGroupPortMappingsWithSyncOp обеспечивает синхронизацию port mapping для binding
// с созданием собственной транзакции и указанной операцией синхронизации
func (s *NetguardService) SyncAddressGroupPortMappingsWithSyncOp(ctx context.Context, binding models.AddressGroupBinding, syncOp models.SyncOp) error {
	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = s.SyncAddressGroupPortMappingsWithWriter(ctx, writer, binding, syncOp); err != nil {
		return err
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Получаем созданный/обновленный mapping для обработки conditions
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		log.Printf("Failed to get reader for conditions processing: %v", err)
		return nil // Не возвращаем ошибку, так как основная операция успешна
	}
	defer reader.Close()

	// Получаем mapping по AddressGroup ID
	mapping, err := reader.GetAddressGroupPortMappingByID(ctx, binding.AddressGroupRef.ResourceIdentifier)
	if err != nil {
		log.Printf("Failed to get port mapping for conditions processing: %v", err)
		return nil
	}

	s.conditionManager.ProcessAddressGroupPortMappingConditions(ctx, mapping)
	if err := s.conditionManager.saveResourceConditions(ctx, mapping); err != nil {
		log.Printf("Failed to save address group port mapping conditions for %s: %v", mapping.Key(), err)
	}

	return nil
}

// SyncAddressGroupBindings syncs address group bindings
func (s *NetguardService) SyncAddressGroupBindings(ctx context.Context, bindings []models.AddressGroupBinding, scope ports.Scope) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	bindingValidator := validator.GetAddressGroupBindingValidator()

	// Validate all bindings
	for i := range bindings {
		binding := &bindings[i]

		// Check if binding exists
		existingBinding, err := reader.GetAddressGroupBindingByID(ctx, binding.ResourceIdentifier)
		if err == nil {
			// Binding exists - use ValidateForUpdate
			if err := bindingValidator.ValidateForUpdate(ctx, *existingBinding, binding); err != nil {
				return err
			}
		} else {
			// Binding is new - use ValidateForCreation
			if err := bindingValidator.ValidateForCreation(ctx, binding); err != nil {
				return err
			}
		}
	}

	// Создаем единую транзакцию для всех операций
	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// --- NEW LOGIC: update Service.AddressGroups before syncing bindings ---
	// Obtain reader that can see uncommitted writes in this transaction
	txReader, err := s.registry.ReaderFromWriter(ctx, writer)
	if err != nil {
		return errors.Wrap(err, "failed to get transaction reader")
	}
	defer txReader.Close()

	var servicesToUpdate []models.Service
	serviceUpdates := make(map[string]*models.Service) // key = service.Key()

	for _, binding := range bindings {
		service, serr := txReader.GetServiceByID(ctx, binding.ServiceRef.ResourceIdentifier)
		if serr != nil {
			if serr == ports.ErrNotFound {
				log.Printf("Service %s not found for binding %s, skipping", binding.ServiceRef.Key(), binding.Key())
				continue
			}
			return errors.Wrapf(serr, "failed to get service %s", binding.ServiceRef.Key())
		}

		key := service.Key()
		if existing, ok := serviceUpdates[key]; ok {
			service = existing
		} else {
			serviceCopy := *service
			serviceUpdates[key] = &serviceCopy
			service = &serviceCopy
		}

		if s.updateServiceAddressGroups(service, binding.AddressGroupRef, "add") {
			log.Printf("Added AddressGroup %s to Service %s", binding.AddressGroupRef.Key(), service.Key())
		}
	}

	for _, svc := range serviceUpdates {
		servicesToUpdate = append(servicesToUpdate, *svc)
	}

	if len(servicesToUpdate) > 0 {
		if err = writer.SyncServices(ctx, servicesToUpdate, nil, ports.WithSyncOp(models.SyncOpUpsert)); err != nil {
			return errors.Wrap(err, "failed to update services with address groups")
		}
	}

	// Sync bindings
	if err = writer.SyncAddressGroupBindings(ctx, bindings, scope, ports.WithSyncOp(models.SyncOpFullSync)); err != nil {
		return errors.Wrap(err, "failed to sync address group bindings")
	}

	// Синхронизируем port mappings для каждого binding в той же транзакции
	for _, binding := range bindings {
		if err := s.SyncAddressGroupPortMappingsWithWriter(ctx, writer, binding, models.SyncOpFullSync); err != nil {
			return errors.Wrapf(err, "failed to sync port mapping for binding %s", binding.Key())
		}
	}

	// Получаем сервисы, которые нужно обновить
	var serviceIDs = make(map[string]models.ResourceIdentifier)
	for _, binding := range bindings {
		serviceIDs[binding.ServiceRef.Key()] = binding.ServiceRef.ResourceIdentifier
	}

	// Получаем все ServiceAlias, связанные с сервисами из bindings
	var serviceAliasIDs []models.ResourceIdentifier
	err = reader.ListServiceAliases(ctx, func(alias models.ServiceAlias) error {
		for _, serviceID := range serviceIDs {
			if alias.ServiceRef.Key() == serviceID.Key() {
				serviceAliasIDs = append(serviceAliasIDs, alias.ResourceIdentifier)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list service aliases")
	}

	// Получаем все RuleS2S, связанные с найденными ServiceAlias
	var rulesToUpdate []models.RuleS2S
	err = reader.ListRuleS2S(ctx, func(rule models.RuleS2S) error {
		for _, aliasID := range serviceAliasIDs {
			if rule.ServiceLocalRef.Key() == aliasID.Key() || rule.ServiceRef.Key() == aliasID.Key() {
				rulesToUpdate = append(rulesToUpdate, rule)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list rules s2s")
	}

	log.Printf("SyncAddressGroupBindings: Found %d RuleS2S to update for %d bindings", len(rulesToUpdate), len(bindings))
	// Обновляем IE AG AG правила
	if len(rulesToUpdate) > 0 {
		// Получаем reader, который может видеть изменения в текущей транзакции
		txReader, err := s.registry.ReaderFromWriter(ctx, writer)
		if err != nil {
			return errors.Wrap(err, "failed to get transaction reader")
		}
		defer txReader.Close()

		if err = s.updateIEAgAgRulesForRuleS2SWithReader(ctx, writer, txReader, rulesToUpdate, models.SyncOpUpsert); err != nil {
			return errors.Wrap(err, "failed to update IE AG AG rules")
		}
	}

	// Фиксируем все изменения в одной транзакции
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Обработка conditions для bindings
	for i := range bindings {
		s.conditionManager.ProcessAddressGroupBindingConditions(ctx, &bindings[i])
		if err := s.conditionManager.saveResourceConditions(ctx, &bindings[i]); err != nil {
			log.Printf("Failed to save address group binding conditions for %s: %v", bindings[i].Key(), err)
		}
	}

	// Обработка conditions для созданных port mappings
	// Получаем reader для чтения созданных mappings
	reader2, err := s.registry.Reader(ctx)
	if err != nil {
		log.Printf("Failed to get reader for port mapping conditions processing: %v", err)
		return nil // Не возвращаем ошибку, так как основная операция успешна
	}
	defer reader2.Close()

	// Обрабатываем conditions для каждого port mapping, созданного для bindings
	for _, binding := range bindings {
		mapping, err := reader2.GetAddressGroupPortMappingByID(ctx, binding.AddressGroupRef.ResourceIdentifier)
		if err != nil {
			log.Printf("Failed to get port mapping for conditions processing for %s: %v", binding.AddressGroupRef.Key(), err)
			continue
		}

		s.conditionManager.ProcessAddressGroupPortMappingConditions(ctx, mapping)
		if err := s.conditionManager.saveResourceConditions(ctx, mapping); err != nil {
			log.Printf("Failed to save address group port mapping conditions for %s: %v", mapping.Key(), err)
		}
	}

	return nil
}

// syncAddressGroupPortMappings синхронизирует маппинги портов групп адресов с указанной операцией
// Не вызывает Commit() - это должен делать вызывающий метод
func (s *NetguardService) syncAddressGroupPortMappings(ctx context.Context, writer ports.Writer, mappings []models.AddressGroupPortMapping, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

		validator := validation.NewDependencyValidator(reader)
		mappingValidator := validator.GetAddressGroupPortMappingValidator()

		for _, mapping := range mappings {
			existingMapping, err := reader.GetAddressGroupPortMappingByID(ctx, mapping.ResourceIdentifier)
			if err == nil && syncOp != models.SyncOpDelete {
				// Маппинг существует - используем ValidateForUpdate
				if err := mappingValidator.ValidateForUpdate(ctx, *existingMapping, mapping); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound && syncOp != models.SyncOpDelete {
				// Маппинг новый - используем ValidateForCreation
				if err := mappingValidator.ValidateForCreation(ctx, mapping); err != nil {
					return err
				}
			} else if err != nil && err != ports.ErrNotFound {
				// Произошла другая ошибка
				return errors.Wrap(err, "failed to get address group port mapping")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if len(mappings) > 0 {
		var ids []models.ResourceIdentifier
		for _, mapping := range mappings {
			ids = append(ids, mapping.ResourceIdentifier)
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Выполнение операции с указанной опцией
	if err := writer.SyncAddressGroupPortMappings(ctx, mappings, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync address group port mappings")
	}

	// Не вызываем Commit() - это должен делать вызывающий метод

	return nil
}

// SyncMultipleAddressGroupPortMappings syncs multiple address group port mappings
func (s *NetguardService) SyncMultipleAddressGroupPortMappings(ctx context.Context, mappings []models.AddressGroupPortMapping, scope ports.Scope) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	mappingValidator := validator.GetAddressGroupPortMappingValidator()

	// Validate all mappings
	for _, mapping := range mappings {
		// Check if mapping exists
		existingMapping, err := reader.GetAddressGroupPortMappingByID(ctx, mapping.ResourceIdentifier)
		if err == nil {
			// Mapping exists - use ValidateForUpdate
			if err := mappingValidator.ValidateForUpdate(ctx, *existingMapping, mapping); err != nil {
				return err
			}
		} else if err == ports.ErrNotFound {
			// Mapping is new - use ValidateForCreation
			if err := mappingValidator.ValidateForCreation(ctx, mapping); err != nil {
				return err
			}
		} else {
			// Other error occurred
			return errors.Wrap(err, "failed to get address group port mapping")
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// Используем обновленный метод syncAddressGroupPortMappings
	if err = s.syncAddressGroupPortMappings(ctx, writer, mappings, models.SyncOpFullSync); err != nil {
		return err
	}

	// Фиксируем транзакцию
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	for i := range mappings {
		s.conditionManager.ProcessAddressGroupPortMappingConditions(ctx, &mappings[i])
		if err := s.conditionManager.saveResourceConditions(ctx, &mappings[i]); err != nil {
			log.Printf("Failed to save address group port mapping conditions for %s: %v", mappings[i].Key(), err)
		}
	}
	return nil
}

// syncRuleS2S синхронизирует правила s2s с указанной операцией
func (s *NetguardService) syncRuleS2S(ctx context.Context, writer ports.Writer, rules []models.RuleS2S, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

		validator := validation.NewDependencyValidator(reader)
		ruleValidator := validator.GetRuleS2SValidator()

		for _, rule := range rules {
			existingRule, err := reader.GetRuleS2SByID(ctx, rule.ResourceIdentifier)
			if err == nil && syncOp != models.SyncOpDelete {
				// Правило существует - используем ValidateForUpdate
				if err := ruleValidator.ValidateForUpdate(ctx, *existingRule, rule); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound && syncOp != models.SyncOpDelete {
				// Правило новое - используем ValidateForCreation
				if err := ruleValidator.ValidateForCreation(ctx, rule); err != nil {
					return err
				}
			} else if err != nil && err != ports.ErrNotFound {
				// Произошла другая ошибка
				return errors.Wrap(err, "failed to get rule s2s")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if len(rules) > 0 {
		var ids []models.ResourceIdentifier
		for _, rule := range rules {
			ids = append(ids, rule.ResourceIdentifier)
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Если это удаление, используем DeleteRuleS2SByIDs для корректного удаления связанных IE AG AG правил
	if syncOp == models.SyncOpDelete {
		// Собираем ID правил
		var ids []models.ResourceIdentifier
		for _, rule := range rules {
			ids = append(ids, rule.ResourceIdentifier)
		}

		// Используем DeleteRuleS2SByIDs для удаления правил и связанных IE AG AG правил
		return s.DeleteRuleS2SByIDs(ctx, ids)
	}

	// Выполнение операции с указанной опцией для не-удаления
	if err := writer.SyncRuleS2S(ctx, rules, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync rule s2s")
	}

	// Генерация AG AG правил
	log.Printf("syncRuleS2S: Starting IEAgAgRule generation for %d RuleS2S", len(rules))

	// Получаем reader, который видит изменения в текущей транзакции
	txReader, err := s.registry.ReaderFromWriter(ctx, writer)
	if err != nil {
		return errors.Wrap(err, "failed to get transaction reader")
	}
	defer txReader.Close()

	// Создаем карту ожидаемых правил после обновления
	expectedRules := make(map[string]bool)
	var allNewRules []models.IEAgAgRule

	// Генерируем IEAgAgRules для каждого RuleS2S
	for i := range rules {
		log.Printf("syncRuleS2S: Generating IEAgAgRules for RuleS2S %d/%d: %s",
			i+1, len(rules), rules[i].Key())

		ieAgAgRules, err := s.GenerateIEAgAgRulesFromRuleS2SWithReader(ctx, txReader, rules[i])
		if err != nil {
			log.Printf("syncRuleS2S: ERROR - Failed to generate IEAgAgRules for RuleS2S %s: %v",
				rules[i].Key(), err)
			return errors.Wrapf(err, "failed to generate IEAgAgRules for RuleS2S %s", rules[i].Key())
		}

		log.Printf("syncRuleS2S: Generated %d IEAgAgRules for RuleS2S %s",
			len(ieAgAgRules), rules[i].Key())

		// Сохраняем ссылки на созданные правила в RuleS2S
		rules[i].IEAgAgRuleRefs = make([]models.ResourceIdentifier, len(ieAgAgRules))

		// Добавляем сгенерированные правила в карту ожидаемых правил и собираем все новые правила
		for j, ieRule := range ieAgAgRules {
			rules[i].IEAgAgRuleRefs[j] = ieRule.ResourceIdentifier
			expectedRules[ieRule.Key()] = true
			allNewRules = append(allNewRules, ieRule)
			log.Printf("syncRuleS2S: Added IEAgAgRule reference %d/%d: %s -> %s",
				j+1, len(ieAgAgRules), rules[i].Key(), ieRule.Key())
		}

		log.Printf("syncRuleS2S: Updated RuleS2S %s with %d IEAgAgRuleRefs",
			rules[i].Key(), len(rules[i].IEAgAgRuleRefs))
	}

	log.Printf("syncRuleS2S: Generated total %d IEAgAgRules for all RuleS2S", len(allNewRules))

	// Обновляем RuleS2S с новыми ссылками на IE AG AG правила
	log.Printf("syncRuleS2S: Updating RuleS2S with IEAgAgRuleRefs")
	if err := writer.SyncRuleS2S(ctx, rules, scope, ports.WithSyncOp(models.SyncOpUpsert)); err != nil {
		log.Printf("syncRuleS2S: ERROR - Failed to update RuleS2S with references: %v", err)
		return errors.Wrap(err, "failed to update RuleS2S with IEAgAgRule references")
	}
	log.Printf("syncRuleS2S: Successfully updated RuleS2S with IEAgAgRuleRefs")

	// Получаем существующие IE AG AG правила по сохраненным ссылкам
	existingRules := make(map[string]models.IEAgAgRule)
	log.Printf("syncRuleS2S: Checking existing IEAgAgRules")

	// Для каждого RuleS2S получаем связанные с ним IE AG AG правила по сохраненным ссылкам
	for _, rule := range rules {
		// Если у правила есть сохраненные ссылки на IE AG AG правила
		for _, ref := range rule.IEAgAgRuleRefs {
			// Получаем IE AG AG правило по ссылке
			ieRule, err := txReader.GetIEAgAgRuleByID(ctx, ref)
			if err == nil && ieRule != nil {
				// Если правило найдено и не nil, добавляем его в карту существующих правил
				existingRules[ieRule.Key()] = *ieRule
			} else if err != ports.ErrNotFound {
				// Если произошла ошибка, отличная от "не найдено", возвращаем ее
				log.Printf("syncRuleS2S: ERROR - Failed to get existing IEAgAgRule %s: %v", ref.Key(), err)
				return errors.Wrapf(err, "failed to get IE AG AG rule %s", ref.Key())
			}
			// Если правило не найдено или nil, просто пропускаем его
		}
	}
	log.Printf("syncRuleS2S: Found %d existing IEAgAgRules", len(existingRules))

	// Синхронизируем все новые правила за один раз WITH sgroups synchronization
	if len(allNewRules) > 0 {
		log.Printf("syncRuleS2S: Syncing %d new IEAgAgRules", len(allNewRules))
		if err = s.syncIEAgAgRules(ctx, writer, allNewRules, models.SyncOpUpsert); err != nil {
			log.Printf("syncRuleS2S: ERROR - Failed to sync IEAgAgRules: %v", err)
			return errors.Wrap(err, "failed to sync new IEAgAgRules")
		}
		log.Printf("syncRuleS2S: Successfully synced %d IEAgAgRules", len(allNewRules))
	} else {
		log.Printf("syncRuleS2S: No new IEAgAgRules to sync")
	}

	// FIXED: Находим и удаляем устаревшие правила, но ТОЛЬКО если есть новые правила для замены
	// Если новых правил нет (len(allNewRules) == 0), НЕ удаляем существующие правила!
	var obsoleteRules []models.IEAgAgRule
	if len(allNewRules) > 0 {
		// Удаляем obsolete правила ТОЛЬКО если есть новые правила
		for key, rule := range existingRules {
			if !expectedRules[key] {
				obsoleteRules = append(obsoleteRules, rule)
				log.Printf("syncRuleS2S: Found obsolete IEAgAgRule: %s", rule.Key())
			}
		}
	} else {
		// ЕСЛИ НЕТ НОВЫХ ПРАВИЛ, НЕ УДАЛЯЕМ СУЩЕСТВУЮЩИЕ!
		log.Printf("syncRuleS2S: No new rules generated, keeping %d existing IEAgAgRules", len(existingRules))
	}

	if len(obsoleteRules) > 0 {
		log.Printf("syncRuleS2S: Deleting %d obsolete IEAgAgRules", len(obsoleteRules))

		// 📋 STEP 1: Log all obsolete rules that will be deleted
		for i, rule := range obsoleteRules {
			log.Printf("  🗑️ OBSOLETE[%d]: %s (%s/%s %s→%s ports=%s)",
				i, rule.Key(), rule.Transport, rule.Traffic,
				rule.AddressGroupLocal.Key(), rule.AddressGroup.Key(),
				rule.Ports[0].Destination)
		}

		// 🚀 STEP 2: CRITICAL - Sync DELETE operation with sgroups FIRST!
		log.Printf("🔄 syncRuleS2S: Syncing DELETE operation with sgroups for %d obsolete rules", len(obsoleteRules))
		if err = s.syncIEAgAgRules(ctx, writer, obsoleteRules, models.SyncOpDelete); err != nil {
			log.Printf("❌ syncRuleS2S: Failed to sync obsolete rules deletion with sgroups: %v", err)
			return errors.Wrap(err, "failed to sync obsolete IEAgAgRules deletion with sgroups")
		}
		log.Printf("✅ syncRuleS2S: Successfully synced DELETE operation with sgroups")

		// 📋 STEP 3: Delete from database AFTER sgroups sync
		var obsoleteRuleIDs []models.ResourceIdentifier
		for _, rule := range obsoleteRules {
			obsoleteRuleIDs = append(obsoleteRuleIDs, rule.ResourceIdentifier)
		}

		log.Printf("🗃️ syncRuleS2S: Deleting %d obsolete rules from database", len(obsoleteRuleIDs))
		if err = writer.DeleteIEAgAgRulesByIDs(ctx, obsoleteRuleIDs); err != nil {
			log.Printf("❌ syncRuleS2S: Failed to delete obsolete rules from database: %v", err)
			return errors.Wrap(err, "failed to delete obsolete IEAgAgRules from database")
		}
		log.Printf("✅ syncRuleS2S: Successfully deleted %d obsolete rules from database", len(obsoleteRuleIDs))
	} else {
		log.Printf("✅ syncRuleS2S: No obsolete IEAgAgRules to delete")
	}

	if err := writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Process conditions for newly created IEAGAG rules after successful commit
	for i := range allNewRules {
		if err := s.conditionManager.ProcessIEAgAgRuleConditions(ctx, &allNewRules[i]); err != nil {
			log.Printf("Failed to process IEAgAgRule conditions for %s: %v", allNewRules[i].Key(), err)
		}
		if err := s.conditionManager.saveResourceConditions(ctx, &allNewRules[i]); err != nil {
			log.Printf("Failed to save IEAgAgRule conditions for %s: %v", allNewRules[i].Key(), err)
		}
	}

	return nil
}

// SyncRuleS2S syncs rule s2s
func (s *NetguardService) SyncRuleS2S(ctx context.Context, rules []models.RuleS2S, scope ports.Scope) error {
	log.Printf("SyncRuleS2S: Starting sync of %d RuleS2S", len(rules))

	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	ruleValidator := validator.GetRuleS2SValidator()

	// Validate all rules
	log.Printf("SyncRuleS2S: Validating %d rules", len(rules))
	for i, rule := range rules {
		log.Printf("SyncRuleS2S: Validating rule %d/%d: %s", i+1, len(rules), rule.Key())

		// Check if rule exists
		existingRule, err := reader.GetRuleS2SByID(ctx, rule.ResourceIdentifier)
		if err == nil {
			log.Printf("SyncRuleS2S: Rule %s exists, validating for update", rule.Key())
			// Rule exists - use ValidateForUpdate
			if err := ruleValidator.ValidateForUpdate(ctx, *existingRule, rule); err != nil {
				log.Printf("SyncRuleS2S: Validation failed for existing rule %s: %v", rule.Key(), err)
				return err
			}
		} else if err == ports.ErrNotFound {
			log.Printf("SyncRuleS2S: Rule %s is new, validating for creation", rule.Key())
			// Rule is new - use ValidateForCreation
			if err := ruleValidator.ValidateForCreation(ctx, rule); err != nil {
				log.Printf("SyncRuleS2S: Validation failed for new rule %s: %v", rule.Key(), err)
				return err
			}
		} else {
			log.Printf("SyncRuleS2S: Error checking rule %s: %v", rule.Key(), err)
			// Other error occurred
			return errors.Wrap(err, "failed to get rule s2s")
		}
	}
	log.Printf("SyncRuleS2S: All rules validated successfully")

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			log.Printf("SyncRuleS2S: Aborting transaction due to error: %v", err)
			writer.Abort()
		}
	}()

	// ИЗМЕНЕНИЕ: Используем приватный метод syncRuleS2S для правильной обработки IEAgAgRule
	log.Printf("SyncRuleS2S: Calling syncRuleS2S with FullSync operation")
	if err = s.syncRuleS2S(ctx, writer, rules, models.SyncOpFullSync); err != nil {
		log.Printf("SyncRuleS2S: syncRuleS2S failed: %v", err)
		return errors.Wrap(err, "failed to sync rule s2s")
	}

	if err = writer.Commit(); err != nil {
		log.Printf("SyncRuleS2S: Failed to commit transaction: %v", err)
		return errors.Wrap(err, "failed to commit")
	}
	log.Printf("SyncRuleS2S: Transaction committed successfully")

	// Process conditions для правил
	log.Printf("SyncRuleS2S: Processing conditions for %d rules", len(rules))
	for i := range rules {
		log.Printf("SyncRuleS2S: Processing conditions for rule %s", rules[i].Key())
		s.conditionManager.ProcessRuleS2SConditions(ctx, &rules[i])
		if err := s.conditionManager.saveResourceConditions(ctx, &rules[i]); err != nil {
			log.Printf("SyncRuleS2S: Failed to save conditions for rule %s: %v", rules[i].Key(), err)
		}
	}

	log.Printf("SyncRuleS2S: Successfully synced %d RuleS2S", len(rules))
	return nil
}

// syncServiceAliases синхронизирует алиасы сервисов с указанной операцией
func (s *NetguardService) syncServiceAliases(ctx context.Context, writer ports.Writer, aliases []models.ServiceAlias, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

		validator := validation.NewDependencyValidator(reader)
		aliasValidator := validator.GetServiceAliasValidator()

		for i := range aliases {
			// Используем указатель на элемент слайса, чтобы изменения сохранились
			alias := &aliases[i]

			existingAlias, err := reader.GetServiceAliasByID(ctx, alias.ResourceIdentifier)
			if err == nil && syncOp != models.SyncOpDelete {
				// Алиас существует - используем ValidateForUpdate
				if err := aliasValidator.ValidateForUpdate(ctx, *existingAlias, *alias); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound && syncOp != models.SyncOpDelete {
				// Алиас новый - используем ValidateForCreation
				if err := aliasValidator.ValidateForCreation(ctx, alias); err != nil {
					return err
				}
			} else if err != nil && err != ports.ErrNotFound {
				// Произошла другая ошибка
				return errors.Wrap(err, "failed to get service alias")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if len(aliases) > 0 {
		var ids []models.ResourceIdentifier
		for _, alias := range aliases {
			ids = append(ids, alias.ResourceIdentifier)
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Выполнение операции с указанной опцией
	if err := writer.SyncServiceAliases(ctx, aliases, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync service aliases")
	}

	if err := writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	return nil
}

// CreateServiceAlias создает новый алиас сервиса
func (s *NetguardService) CreateServiceAlias(ctx context.Context, alias models.ServiceAlias) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	aliasValidator := validator.GetServiceAliasValidator()

	// Валидируем алиас перед созданием
	if err := aliasValidator.ValidateForCreation(ctx, &alias); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncServiceAliases(ctx, []models.ServiceAlias{alias}, ports.NewResourceIdentifierScope(alias.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to create service alias")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	// Обработка conditions
	s.conditionManager.ProcessServiceAliasConditions(ctx, &alias)
	if err := s.conditionManager.saveResourceConditions(ctx, &alias); err != nil {
		return errors.Wrap(err, "failed to save service alias conditions")
	}
	return nil
}

// UpdateServiceAlias обновляет существующий алиас сервиса
func (s *NetguardService) UpdateServiceAlias(ctx context.Context, alias models.ServiceAlias) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Получаем старую версию алиаса
	oldAlias, err := reader.GetServiceAliasByID(ctx, alias.ResourceIdentifier)
	if err != nil {
		return errors.Wrap(err, "failed to get existing service alias")
	}

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	aliasValidator := validator.GetServiceAliasValidator()

	// Валидируем алиас перед обновлением
	if err := aliasValidator.ValidateForUpdate(ctx, *oldAlias, alias); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncServiceAliases(ctx, []models.ServiceAlias{alias}, ports.NewResourceIdentifierScope(alias.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to update service alias")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	s.conditionManager.ProcessServiceAliasConditions(ctx, &alias)
	if err := s.conditionManager.saveResourceConditions(ctx, &alias); err != nil {
		return errors.Wrap(err, "failed to save service alias conditions")
	}
	return nil
}

// findRuleS2SForServiceAliases finds all RuleS2S that reference the given service aliases
func (s *NetguardService) findRuleS2SForServiceAliases(ctx context.Context, aliasIDs []models.ResourceIdentifier) ([]models.RuleS2S, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create a map of service alias IDs for quick lookup
	aliasMap := make(map[string]bool)
	for _, id := range aliasIDs {
		aliasMap[id.Key()] = true
	}

	// Find all RuleS2S that reference these service aliases
	var rules []models.RuleS2S
	err = reader.ListRuleS2S(ctx, func(rule models.RuleS2S) error {
		// Check if the rule references any of the service aliases
		if aliasMap[rule.ServiceLocalRef.Key()] || aliasMap[rule.ServiceRef.Key()] {
			rules = append(rules, rule)
		}
		return nil
	}, nil)

	if err != nil {
		return nil, errors.Wrap(err, "failed to list rules")
	}

	return rules, nil
}

// SyncServiceAliases syncs service aliases
func (s *NetguardService) SyncServiceAliases(ctx context.Context, aliases []models.ServiceAlias, scope ports.Scope) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	aliasValidator := validator.GetServiceAliasValidator()

	// Validate all aliases
	for i := range aliases {
		// Используем указатель на элемент слайса, чтобы изменения сохранились
		alias := &aliases[i]

		// Check if alias exists
		existingAlias, err := reader.GetServiceAliasByID(ctx, alias.ResourceIdentifier)
		if err == nil {
			// Alias exists - use ValidateForUpdate
			if err := aliasValidator.ValidateForUpdate(ctx, *existingAlias, *alias); err != nil {
				return err
			}
		} else if err == ports.ErrNotFound {
			// Alias is new - use ValidateForCreation
			if err := aliasValidator.ValidateForCreation(ctx, alias); err != nil {
				return err
			}
		} else {
			// Other error occurred
			return errors.Wrap(err, "failed to get service alias")
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// Find RuleS2S that depend on these ServiceAliases BEFORE making changes
	var aliasIDs []models.ResourceIdentifier
	for _, alias := range aliases {
		aliasIDs = append(aliasIDs, alias.ResourceIdentifier)
	}

	affectedRules, err := s.findRuleS2SForServiceAliases(ctx, aliasIDs)
	if err != nil {
		return errors.Wrap(err, "failed to find affected RuleS2S")
	}

	log.Printf("SyncServiceAliases: Found %d RuleS2S affected by %d ServiceAlias changes", len(affectedRules), len(aliases))

	if err = writer.SyncServiceAliases(ctx, aliases, scope, ports.WithSyncOp(models.SyncOpFullSync)); err != nil {
		return errors.Wrap(err, "failed to sync service aliases")
	}

	// Update IEAgAgRules for affected RuleS2S (ServiceAlias changes can affect Service→AddressGroups→Ports)
	if len(affectedRules) > 0 {
		// Get reader that can see changes in current transaction
		txReader, err := s.registry.ReaderFromWriter(ctx, writer)
		if err != nil {
			return errors.Wrap(err, "failed to get transaction reader")
		}
		defer txReader.Close()

		if err = s.updateIEAgAgRulesForRuleS2SWithReader(ctx, writer, txReader, affectedRules, models.SyncOpFullSync); err != nil {
			return errors.Wrap(err, "failed to update IEAgAgRules for affected RuleS2S")
		}

		log.Printf("SyncServiceAliases: Updated IEAgAgRules for %d affected RuleS2S", len(affectedRules))
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Process conditions after successful commit
	for i := range aliases {
		s.conditionManager.ProcessServiceAliasConditions(ctx, &aliases[i])
		if err := s.conditionManager.saveResourceConditions(ctx, &aliases[i]); err != nil {
			log.Printf("Failed to save service alias conditions for %s: %v", aliases[i].Key(), err)
		}
	}
	return nil
}

// CreateAddressGroupBindingPolicy создает новую политику привязки группы адресов
func (s *NetguardService) CreateAddressGroupBindingPolicy(ctx context.Context, policy models.AddressGroupBindingPolicy) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	policyValidator := validator.GetAddressGroupBindingPolicyValidator()

	// Валидируем политику перед созданием
	if err := policyValidator.ValidateForCreation(ctx, &policy); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroupBindingPolicies(ctx, []models.AddressGroupBindingPolicy{policy}, ports.NewResourceIdentifierScope(policy.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to create address group binding policy")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	// Обработка conditions
	s.conditionManager.ProcessAddressGroupBindingPolicyConditions(ctx, &policy)
	if err := s.conditionManager.saveResourceConditions(ctx, &policy); err != nil {
		return errors.Wrap(err, "failed to save address group binding policy conditions")
	}
	return nil
}

// UpdateAddressGroupBindingPolicy обновляет существующую политику привязки группы адресов
func (s *NetguardService) UpdateAddressGroupBindingPolicy(ctx context.Context, policy models.AddressGroupBindingPolicy) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Получаем старую версию политики
	oldPolicy, err := reader.GetAddressGroupBindingPolicyByID(ctx, policy.ResourceIdentifier)
	if err != nil {
		return errors.Wrap(err, "failed to get existing address group binding policy")
	}

	// Создаем валидатор
	validator := validation.NewDependencyValidator(reader)
	policyValidator := validator.GetAddressGroupBindingPolicyValidator()

	// Валидируем политику перед обновлением
	if err := policyValidator.ValidateForUpdate(ctx, *oldPolicy, &policy); err != nil {
		return err
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroupBindingPolicies(ctx, []models.AddressGroupBindingPolicy{policy}, ports.NewResourceIdentifierScope(policy.ResourceIdentifier)); err != nil {
		return errors.Wrap(err, "failed to update address group binding policy")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	s.conditionManager.ProcessAddressGroupBindingPolicyConditions(ctx, &policy)
	if err := s.conditionManager.saveResourceConditions(ctx, &policy); err != nil {
		return errors.Wrap(err, "failed to save address group binding policy conditions")
	}
	return nil
}

// GetSyncStatus returns the status of the last synchronization
func (s *NetguardService) GetSyncStatus(ctx context.Context) (*models.SyncStatus, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	return reader.GetSyncStatus(ctx)
}

// GetServiceByID returns a service by ID
func (s *NetguardService) GetServiceByID(ctx context.Context, id models.ResourceIdentifier) (*models.Service, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	service, err := reader.GetServiceByID(ctx, id)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get service")
	}

	return service, nil
}

// GetAddressGroupByID returns an address group by ID
func (s *NetguardService) GetAddressGroupByID(ctx context.Context, id models.ResourceIdentifier) (*models.AddressGroup, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	addressGroup, err := reader.GetAddressGroupByID(ctx, id)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get address group")
	}

	return addressGroup, nil
}

// GetAddressGroupBindingByID returns an address group binding by ID
func (s *NetguardService) GetAddressGroupBindingByID(ctx context.Context, id models.ResourceIdentifier) (*models.AddressGroupBinding, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	binding, err := reader.GetAddressGroupBindingByID(ctx, id)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get address group binding")
	}

	return binding, nil
}

// GetAddressGroupPortMappingByID returns an address group port mapping by ID
func (s *NetguardService) GetAddressGroupPortMappingByID(ctx context.Context, id models.ResourceIdentifier) (*models.AddressGroupPortMapping, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	mapping, err := reader.GetAddressGroupPortMappingByID(ctx, id)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get address group port mapping")
	}

	return mapping, nil
}

// GetRuleS2SByID returns a rule s2s by ID
func (s *NetguardService) GetRuleS2SByID(ctx context.Context, id models.ResourceIdentifier) (*models.RuleS2S, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	rule, err := reader.GetRuleS2SByID(ctx, id)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get rule s2s")
	}

	return rule, nil
}

// GetServiceAliasByID returns a service alias by ID
func (s *NetguardService) GetServiceAliasByID(ctx context.Context, id models.ResourceIdentifier) (*models.ServiceAlias, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	alias, err := reader.GetServiceAliasByID(ctx, id)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get service alias")
	}

	return alias, nil
}

// GetAddressGroupBindingPolicyByID returns an address group binding policy by ID
func (s *NetguardService) GetAddressGroupBindingPolicyByID(ctx context.Context, id models.ResourceIdentifier) (*models.AddressGroupBindingPolicy, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	policy, err := reader.GetAddressGroupBindingPolicyByID(ctx, id)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get address group binding policy")
	}

	return policy, nil
}

// CreateNetwork creates a new Network with business logic
func (s *NetguardService) CreateNetwork(ctx context.Context, network models.Network) error {
	return s.networkService.CreateNetwork(ctx, &network)
}

// UpdateNetwork updates an existing Network with business logic
func (s *NetguardService) UpdateNetwork(ctx context.Context, network models.Network) error {
	return s.networkService.UpdateNetwork(ctx, &network)
}

// DeleteNetwork deletes a Network with cleanup logic
func (s *NetguardService) DeleteNetwork(ctx context.Context, id models.ResourceIdentifier) error {
	return s.networkService.DeleteNetwork(ctx, id)
}

// GetNetworkByID returns a network by ID
func (s *NetguardService) GetNetworkByID(ctx context.Context, id models.ResourceIdentifier) (*models.Network, error) {
	return s.networkService.GetNetwork(ctx, id)
}

// CreateNetworkBinding creates a new NetworkBinding with business logic
func (s *NetguardService) CreateNetworkBinding(ctx context.Context, binding models.NetworkBinding) error {
	return s.networkBindingService.CreateNetworkBinding(ctx, &binding)
}

// UpdateNetworkBinding updates an existing NetworkBinding with business logic
func (s *NetguardService) UpdateNetworkBinding(ctx context.Context, binding models.NetworkBinding) error {
	return s.networkBindingService.UpdateNetworkBinding(ctx, &binding)
}

// DeleteNetworkBinding deletes a NetworkBinding with cleanup logic
func (s *NetguardService) DeleteNetworkBinding(ctx context.Context, id models.ResourceIdentifier) error {
	return s.networkBindingService.DeleteNetworkBinding(ctx, id)
}

// GetNetworkBindingByID returns a network binding by ID
func (s *NetguardService) GetNetworkBindingByID(ctx context.Context, id models.ResourceIdentifier) (*models.NetworkBinding, error) {
	return s.networkBindingService.GetNetworkBinding(ctx, id)
}

// GetServicesByIDs returns services by IDs
func (s *NetguardService) GetServicesByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.Service, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var services []models.Service
	err = reader.ListServices(ctx, func(service models.Service) error {
		services = append(services, service)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list services")
	}

	return services, nil
}

// GetAddressGroupsByIDs returns address groups by IDs
func (s *NetguardService) GetAddressGroupsByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.AddressGroup, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var addressGroups []models.AddressGroup
	err = reader.ListAddressGroups(ctx, func(addressGroup models.AddressGroup) error {
		addressGroups = append(addressGroups, addressGroup)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list address groups")
	}

	return addressGroups, nil
}

// GetAddressGroupBindingsByIDs returns address group bindings by IDs
func (s *NetguardService) GetAddressGroupBindingsByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.AddressGroupBinding, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var bindings []models.AddressGroupBinding
	err = reader.ListAddressGroupBindings(ctx, func(binding models.AddressGroupBinding) error {
		bindings = append(bindings, binding)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list address group bindings")
	}

	return bindings, nil
}

// GetAddressGroupPortMappingsByIDs returns address group port mappings by IDs
func (s *NetguardService) GetAddressGroupPortMappingsByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.AddressGroupPortMapping, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var mappings []models.AddressGroupPortMapping
	err = reader.ListAddressGroupPortMappings(ctx, func(mapping models.AddressGroupPortMapping) error {
		mappings = append(mappings, mapping)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list address group port mappings")
	}

	return mappings, nil
}

// GetRuleS2SByIDs returns rules s2s by IDs
func (s *NetguardService) GetRuleS2SByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.RuleS2S, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var rules []models.RuleS2S
	err = reader.ListRuleS2S(ctx, func(rule models.RuleS2S) error {
		rules = append(rules, rule)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list rules s2s")
	}

	return rules, nil
}

// GetServiceAliasesByIDs returns service aliases by IDs
func (s *NetguardService) GetServiceAliasesByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.ServiceAlias, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var aliases []models.ServiceAlias
	err = reader.ListServiceAliases(ctx, func(alias models.ServiceAlias) error {
		aliases = append(aliases, alias)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list service aliases")
	}

	return aliases, nil
}

// GetAddressGroupBindingPoliciesByIDs returns address group binding policies by IDs
func (s *NetguardService) GetAddressGroupBindingPoliciesByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.AddressGroupBindingPolicy, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var policies []models.AddressGroupBindingPolicy
	err = reader.ListAddressGroupBindingPolicies(ctx, func(policy models.AddressGroupBindingPolicy) error {
		policies = append(policies, policy)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list address group binding policies")
	}

	return policies, nil
}

// GetNetworksByIDs returns networks by IDs
func (s *NetguardService) GetNetworksByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.Network, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var networks []models.Network
	err = reader.ListNetworks(ctx, func(network models.Network) error {
		networks = append(networks, network)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list networks")
	}

	return networks, nil
}

// GetNetworkBindingsByIDs returns network bindings by IDs
func (s *NetguardService) GetNetworkBindingsByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.NetworkBinding, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var bindings []models.NetworkBinding
	err = reader.ListNetworkBindings(ctx, func(binding models.NetworkBinding) error {
		bindings = append(bindings, binding)
		return nil
	}, ports.NewResourceIdentifierScope(ids...))

	if err != nil {
		return nil, errors.Wrap(err, "failed to list network bindings")
	}

	return bindings, nil
}

// DeleteServicesByIDs deletes services by IDs with cascade deletion of dependencies
func (s *NetguardService) DeleteServicesByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator for aliases
	validator := validation.NewDependencyValidator(reader)
	aliasValidator := validator.GetServiceAliasValidator()

	// For each service, check its aliases
	for _, id := range ids {
		// Find all aliases of the service
		var serviceAliases []models.ServiceAlias
		err = reader.ListServiceAliases(ctx, func(alias models.ServiceAlias) error {
			if alias.ServiceRef.Key() == id.Key() {
				serviceAliases = append(serviceAliases, alias)
			}
			return nil
		}, nil)

		if err != nil {
			return errors.Wrap(err, "failed to list service aliases")
		}

		// If the service has aliases, check if they have related rules s2s
		for _, alias := range serviceAliases {
			// Check alias dependencies
			if err := aliasValidator.CheckDependencies(ctx, alias.ResourceIdentifier); err != nil {
				// If the alias has dependencies (rules s2s), return an error
				return errors.Wrapf(err, "service %s has alias %s with dependencies", id.Key(), alias.Key())
			}
		}
	}

	// Get all bindings related to the services being deleted
	var bindingsToDelete []models.ResourceIdentifier
	err = reader.ListAddressGroupBindings(ctx, func(binding models.AddressGroupBinding) error {
		for _, id := range ids {
			if binding.ServiceRef.Key() == id.Key() {
				bindingsToDelete = append(bindingsToDelete, binding.ResourceIdentifier)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list address group bindings")
	}

	// Get all service aliases related to the services being deleted
	var serviceAliases []models.ServiceAlias
	err = reader.ListServiceAliases(ctx, func(alias models.ServiceAlias) error {
		for _, id := range ids {
			if alias.ServiceRef.Key() == id.Key() {
				serviceAliases = append(serviceAliases, alias)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list service aliases")
	}

	// Get alias IDs
	var aliasIDs []models.ResourceIdentifier
	for _, alias := range serviceAliases {
		aliasIDs = append(aliasIDs, alias.ResourceIdentifier)
	}

	// Get all RuleS2S rules related to aliases of the services being deleted
	var rulesToDelete []models.ResourceIdentifier
	err = reader.ListRuleS2S(ctx, func(rule models.RuleS2S) error {
		for _, alias := range serviceAliases {
			if rule.ServiceLocalRef.Key() == alias.ResourceIdentifier.Key() ||
				rule.ServiceRef.Key() == alias.ResourceIdentifier.Key() {
				rulesToDelete = append(rulesToDelete, rule.ResourceIdentifier)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list rules s2s")
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// 1. Delete bindings
	if len(bindingsToDelete) > 0 {
		log.Println("Deleting", len(bindingsToDelete), "bindings for services")
		if err = s.DeleteAddressGroupBindingsByIDs(ctx, bindingsToDelete); err != nil {
			return errors.Wrap(err, "failed to delete address group bindings")
		}
	}

	// 2. Delete RuleS2S rules and related IEAGAG rules
	if len(rulesToDelete) > 0 {
		log.Println("Deleting", len(rulesToDelete), "RuleS2S for services")
		if err = s.DeleteRuleS2SByIDs(ctx, rulesToDelete); err != nil {
			return errors.Wrap(err, "failed to delete rules s2s")
		}
	}

	// 3. Delete service aliases
	if len(aliasIDs) > 0 {
		log.Println("Deleting", len(aliasIDs), "service aliases")
		if err = writer.DeleteServiceAliasesByIDs(ctx, aliasIDs); err != nil {
			return errors.Wrap(err, "failed to delete service aliases")
		}
	}

	// 4. Delete services
	log.Println("Deleting", len(ids), "services")
	if err = writer.DeleteServicesByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete services")
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	return nil
}

// DeleteAddressGroupsByIDs deletes address groups by IDs with cascade deletion of dependencies
func (s *NetguardService) DeleteAddressGroupsByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Note: We're not checking dependencies here as we're handling them with cascade deletion

	// Get address groups that will be deleted
	var addressGroups []models.AddressGroup
	for _, id := range ids {
		ag, err := reader.GetAddressGroupByID(ctx, id)
		if err != nil {
			continue // Skip if group doesn't exist
		}
		addressGroups = append(addressGroups, *ag)
	}

	// Get all bindings related to the address groups being deleted
	var bindingsToDelete []models.ResourceIdentifier
	err = reader.ListAddressGroupBindings(ctx, func(binding models.AddressGroupBinding) error {
		for _, ag := range addressGroups {
			if binding.AddressGroupRef.Key() == ag.ResourceIdentifier.Key() {
				bindingsToDelete = append(bindingsToDelete, binding.ResourceIdentifier)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list address group bindings")
	}

	// Get all services that reference the address groups being deleted
	var servicesToUpdate []models.Service
	err = reader.ListServices(ctx, func(service models.Service) error {
		serviceUpdated := false
		updatedAGs := make([]models.AddressGroupRef, 0, len(service.AddressGroups))

		// Filter address groups, keeping only those that won't be deleted
		for _, agRef := range service.AddressGroups {
			shouldKeep := true
			for _, id := range ids {
				if agRef.Key() == id.Key() {
					shouldKeep = false
					serviceUpdated = true
					break
				}
			}
			if shouldKeep {
				updatedAGs = append(updatedAGs, agRef)
			}
		}

		if serviceUpdated {
			updatedService := service
			updatedService.AddressGroups = updatedAGs
			servicesToUpdate = append(servicesToUpdate, updatedService)
		}

		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list services")
	}

	// Get all NetworkBindings that reference the address groups being deleted
	var networkBindingsToDelete []models.ResourceIdentifier
	err = reader.ListNetworkBindings(ctx, func(binding models.NetworkBinding) error {
		for _, id := range ids {
			// ObjectReference содержит только Name, сравниваем по имени
			// NetworkBinding и AddressGroup должны быть в одном namespace
			if binding.AddressGroupRef.Name == id.Name && binding.Namespace == id.Namespace {
				networkBindingsToDelete = append(networkBindingsToDelete, binding.ResourceIdentifier)
				log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Found NetworkBinding %s referencing AddressGroup %s", binding.Key(), id.Key())
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list network bindings")
	}

	// Get all IE AG AG rules that reference the address groups being deleted
	var ieRulesToDelete []models.ResourceIdentifier
	err = reader.ListIEAgAgRules(ctx, func(rule models.IEAgAgRule) error {
		for _, id := range ids {
			if rule.AddressGroupLocal.Key() == id.Key() || rule.AddressGroup.Key() == id.Key() {
				ieRulesToDelete = append(ieRulesToDelete, rule.ResourceIdentifier)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list IE AG AG rules")
	}

	// Start transaction for all operations
	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// 1. Update Networks to clear binding references BEFORE deleting NetworkBindings
	var networksToUpdate []models.Network
	if len(networkBindingsToDelete) > 0 {
		log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Processing %d NetworkBindings for deletion", len(networkBindingsToDelete))

		// Получаем NetworkBindings которые будут удалены
		for _, bindingID := range networkBindingsToDelete {
			binding, err := reader.GetNetworkBindingByID(ctx, bindingID)
			if err != nil {
				if err == ports.ErrNotFound {
					log.Printf("⚠️  DEBUG: DeleteAddressGroupsByIDs - NetworkBinding %s not found, skipping", bindingID.Key())
					continue
				}
				return errors.Wrapf(err, "failed to get network binding %s", bindingID.Key())
			}

			// Находим связанную Network и обновляем её
			networkRef := models.ResourceIdentifier{Name: binding.NetworkRef.Name, Namespace: binding.Namespace}
			network, err := reader.GetNetworkByID(ctx, networkRef)
			if err != nil {
				if err == ports.ErrNotFound {
					log.Printf("⚠️  DEBUG: DeleteAddressGroupsByIDs - Network %s not found, skipping", networkRef.Key())
					continue
				}
				return errors.Wrapf(err, "failed to get network %s", networkRef.Key())
			}

			log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Clearing binding references in Network %s", network.Key())

			// Очищаем binding references
			network.BindingRef = nil
			network.AddressGroupRef = nil
			network.IsBound = false
			network.Meta.TouchOnWrite(fmt.Sprintf("unbinding-ag-deletion-%d", time.Now().UnixNano()))

			networksToUpdate = append(networksToUpdate, *network)
		}

		// Обновляем Network в транзакции
		if len(networksToUpdate) > 0 {
			log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Updating %d Networks to clear binding references", len(networksToUpdate))
			if err = writer.SyncNetworks(ctx, networksToUpdate, nil, ports.WithSyncOp(models.SyncOpUpsert)); err != nil {
				return errors.Wrap(err, "failed to update networks to clear binding references")
			}
		}

		// Теперь удаляем NetworkBindings
		log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Deleting %d NetworkBindings for address groups", len(networkBindingsToDelete))
		if err = writer.DeleteNetworkBindingsByIDs(ctx, networkBindingsToDelete); err != nil {
			return errors.Wrap(err, "failed to delete network bindings")
		}
	}

	// 1.5. Удаляем Network из AddressGroups перед удалением самих AddressGroups
	var updatedAddressGroups []models.AddressGroup
	if len(networksToUpdate) > 0 {
		log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Removing Networks from AddressGroups before deletion")

		for _, addressGroup := range addressGroups {
			// Для каждой AddressGroup удаляем Networks которые были обновлены
			originalNetworkCount := len(addressGroup.Networks)
			var updatedNetworks []models.NetworkItem

			for _, networkItem := range addressGroup.Networks {
				// Проверяем, есть ли эта сеть среди обновленных
				shouldRemove := false
				for _, updatedNetwork := range networksToUpdate {
					networkName := fmt.Sprintf("%s/%s", updatedNetwork.Namespace, updatedNetwork.Name)
					if networkItem.Name == networkName {
						shouldRemove = true
						log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Removing Network %s from AddressGroup %s", networkName, addressGroup.Key())
						break
					}
				}

				if !shouldRemove {
					updatedNetworks = append(updatedNetworks, networkItem)
				}
			}

			if len(updatedNetworks) != originalNetworkCount {
				addressGroup.Networks = updatedNetworks
				addressGroup.Meta.TouchOnWrite(fmt.Sprintf("network-removal-%d", time.Now().UnixNano()))
				updatedAddressGroups = append(updatedAddressGroups, addressGroup)
				log.Printf("✅ DEBUG: DeleteAddressGroupsByIDs - Removed %d Networks from AddressGroup %s", originalNetworkCount-len(updatedNetworks), addressGroup.Key())
			}
		}

		// Обновляем AddressGroups если были изменения
		if len(updatedAddressGroups) > 0 {
			log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Updating %d AddressGroups to remove Networks", len(updatedAddressGroups))
			if err = writer.SyncAddressGroups(ctx, updatedAddressGroups, nil, ports.WithSyncOp(models.SyncOpUpsert)); err != nil {
				return errors.Wrap(err, "failed to update address groups to remove networks")
			}
		}
	}

	// 2. Delete AddressGroupBindings
	if len(bindingsToDelete) > 0 {
		log.Println("Deleting", len(bindingsToDelete), "bindings for address groups")
		if err = writer.DeleteAddressGroupBindingsByIDs(ctx, bindingsToDelete); err != nil {
			return errors.Wrap(err, "failed to delete address group bindings")
		}
	}

	// 3. Update services, removing references to deleted address groups
	if len(servicesToUpdate) > 0 {
		log.Println("Updating", len(servicesToUpdate), "services to remove references to deleted address groups")
		if err = writer.SyncServices(ctx, servicesToUpdate, nil, ports.WithSyncOp(models.SyncOpUpsert)); err != nil {
			return errors.Wrap(err, "failed to update services")
		}
	}

	// 4. Delete IE AG AG rules related to the address groups being deleted
	if len(ieRulesToDelete) > 0 {
		log.Println("Deleting", len(ieRulesToDelete), "IE AG AG rules for address groups")
		if err = writer.DeleteIEAgAgRulesByIDs(ctx, ieRulesToDelete); err != nil {
			return errors.Wrap(err, "failed to delete IE AG AG rules")
		}
	}

	// 5. Delete address groups
	log.Println("Deleting", len(ids), "address groups")
	if err = writer.DeleteAddressGroupsByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete address groups")
	}

	// Commit transaction
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Синхронизация обновленных AddressGroups с sgroups (перед удалением)
	if len(updatedAddressGroups) > 0 {
		log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Starting sgroups synchronization for %d updated AddressGroups", len(updatedAddressGroups))
		s.syncAddressGroupsWithSGroups(ctx, updatedAddressGroups, types.SyncOperationUpsert)
		log.Printf("✅ DEBUG: DeleteAddressGroupsByIDs - Completed updated AddressGroups sgroups synchronization")
	}

	// Синхронизация с sgroups после успешного удаления из БД
	if len(addressGroups) > 0 {
		log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Starting sgroups synchronization for %d deleted AddressGroups", len(addressGroups))
		s.syncAddressGroupsWithSGroups(ctx, addressGroups, types.SyncOperationDelete)
		log.Printf("✅ DEBUG: DeleteAddressGroupsByIDs - Completed AddressGroups sgroups synchronization")
	}

	// Синхронизация обновленных Network с sgroups
	if len(networksToUpdate) > 0 {
		log.Printf("🔧 DEBUG: DeleteAddressGroupsByIDs - Starting sgroups synchronization for %d updated Networks", len(networksToUpdate))
		networkSyncResults := s.syncNetworksWithSGroups(ctx, networksToUpdate, types.SyncOperationUpsert)
		log.Printf("✅ DEBUG: DeleteAddressGroupsByIDs - Completed Networks sgroups synchronization")

		// Обрабатываем условия для обновленных Network с результатами синхронизации
		for i := range networksToUpdate {
			network := &networksToUpdate[i]
			syncResult := networkSyncResults[network.GetSyncKey()]
			s.conditionManager.ProcessNetworkConditions(ctx, network, syncResult)
			if err := s.conditionManager.saveResourceConditions(ctx, network); err != nil {
				log.Printf("Failed to save network conditions for %s: %v", network.Key(), err)
			}
		}
	}

	return nil
}

// DeleteAddressGroupBindingsByIDs deletes address group bindings by IDs with cascade deletion of dependencies
func (s *NetguardService) DeleteAddressGroupBindingsByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Get bindings that will be deleted
	var bindings []models.AddressGroupBinding
	for _, id := range ids {
		binding, err := reader.GetAddressGroupBindingByID(ctx, id)
		if err != nil || binding == nil {
			continue // Skip if binding doesn't exist
		}
		bindings = append(bindings, *binding)
	}

	// Get services that need to be updated
	var serviceIDs = make(map[string]models.ResourceIdentifier)
	for _, binding := range bindings {
		serviceIDs[binding.ServiceRef.Key()] = binding.ServiceRef.ResourceIdentifier
	}

	// Get all RuleS2S related to services from bindings
	var serviceAliasIDs []models.ResourceIdentifier
	err = reader.ListServiceAliases(ctx, func(alias models.ServiceAlias) error {
		for _, serviceID := range serviceIDs {
			if alias.ServiceRef.Key() == serviceID.Key() {
				serviceAliasIDs = append(serviceAliasIDs, alias.ResourceIdentifier)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list service aliases")
	}

	var rulesToUpdate []models.RuleS2S
	err = reader.ListRuleS2S(ctx, func(rule models.RuleS2S) error {
		for _, aliasID := range serviceAliasIDs {
			if rule.ServiceLocalRef.Key() == aliasID.Key() || rule.ServiceRef.Key() == aliasID.Key() {
				rulesToUpdate = append(rulesToUpdate, rule)
				break
			}
		}
		return nil
	}, nil)
	if err != nil {
		return errors.Wrap(err, "failed to list rules s2s")
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// --- NEW LOGIC: update Service.AddressGroups before deleting bindings ---
	// Obtain transactional reader
	txReader, err := s.registry.ReaderFromWriter(ctx, writer)
	if err != nil {
		return errors.Wrap(err, "failed to get transaction reader")
	}
	defer txReader.Close()

	var servicesToUpdate []models.Service
	serviceUpdates := make(map[string]*models.Service)

	for _, binding := range bindings {
		service, serr := txReader.GetServiceByID(ctx, binding.ServiceRef.ResourceIdentifier)
		if serr != nil {
			if serr == ports.ErrNotFound {
				continue
			}
			return errors.Wrapf(serr, "failed to get service %s", binding.ServiceRef.Key())
		}
		key := service.Key()
		if existing, ok := serviceUpdates[key]; ok {
			service = existing
		} else {
			serviceCopy := *service
			serviceUpdates[key] = &serviceCopy
			service = &serviceCopy
		}

		if s.updateServiceAddressGroups(service, binding.AddressGroupRef, "remove") {
			log.Printf("Removed AddressGroup %s from Service %s", binding.AddressGroupRef.Key(), service.Key())
		}
	}

	for _, svc := range serviceUpdates {
		servicesToUpdate = append(servicesToUpdate, *svc)
	}

	if len(servicesToUpdate) > 0 {
		if err = writer.SyncServices(ctx, servicesToUpdate, nil, ports.WithSyncOp(models.SyncOpUpsert)); err != nil {
			return errors.Wrap(err, "failed to update services after binding deletion")
		}
	}

	// Delete bindings
	if err = writer.DeleteAddressGroupBindingsByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete address group bindings")
	}

	// Update port mappings for each deleted binding
	for _, binding := range bindings {
		// Check if there are other bindings for the same address group
		hasOtherBindings := false
		err = reader.ListAddressGroupBindings(ctx, func(b models.AddressGroupBinding) error {
			if b.AddressGroupRef.Key() == binding.AddressGroupRef.Key() && b.Key() != binding.Key() {
				hasOtherBindings = true
			}
			return nil
		}, nil)

		if err != nil {
			return errors.Wrap(err, "failed to check for other bindings")
		}

		// If there are no other bindings, delete port mapping
		if !hasOtherBindings {
			if err = writer.DeleteAddressGroupPortMappingsByIDs(ctx, []models.ResourceIdentifier{binding.AddressGroupRef.ResourceIdentifier}); err != nil {
				return errors.Wrap(err, "failed to delete address group port mappings")
			}
		} else {
			// Otherwise update port mapping, removing the service
			portMapping, err := reader.GetAddressGroupPortMappingByID(ctx, binding.AddressGroupRef.ResourceIdentifier)
			if err != nil {
				continue // Skip if port mapping doesn't exist
			}

			// Remove service from port mapping
			delete(portMapping.AccessPorts, binding.ServiceRef)

			// Update port mapping
			if err = writer.SyncAddressGroupPortMappings(
				ctx,
				[]models.AddressGroupPortMapping{*portMapping},
				ports.NewResourceIdentifierScope(portMapping.ResourceIdentifier),
				ports.WithSyncOp(models.SyncOpUpsert),
			); err != nil {
				return errors.Wrap(err, "failed to update address group port mappings")
			}
		}
	}

	// Update RuleS2S and related IEAGAG rules
	if len(rulesToUpdate) > 0 {
		// Get reader that can see changes in the current transaction
		txReader, err := s.registry.ReaderFromWriter(ctx, writer)
		if err != nil {
			return errors.Wrap(err, "failed to get transaction reader")
		}
		defer txReader.Close()

		if err = s.updateIEAgAgRulesForRuleS2SWithReader(ctx, writer, txReader, rulesToUpdate, models.SyncOpUpsert); err != nil {
			return errors.Wrap(err, "failed to update IE AG AG rules")
		}
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	return nil
}

// DeleteAddressGroupPortMappingsByIDs deletes address group port mappings by IDs
func (s *NetguardService) DeleteAddressGroupPortMappingsByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	// Note: Address group port mappings don't have dependencies, so we don't need to check for them
	// However, we could add validation to ensure the mappings exist before deleting them

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.DeleteAddressGroupPortMappingsByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete address group port mappings")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	return nil
}

// DeleteRuleS2SByIDs deletes rules s2s by IDs
func (s *NetguardService) DeleteRuleS2SByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	log.Printf("DeleteRuleS2SByIDs: Starting deletion of %d RuleS2S", len(ids))

	// Get a reader to fetch RuleS2S objects
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Use map to deduplicate IEAgAgRule IDs
	ieAgAgRulesToDelete := make(map[string]models.ResourceIdentifier)

	// For each RuleS2S, collect all related IEAgAgRule IDs
	for _, id := range ids {
		log.Printf("DeleteRuleS2SByIDs: Processing RuleS2S %s", id.Key())

		ruleS2S, err := reader.GetRuleS2SByID(ctx, id)
		if err != nil || ruleS2S == nil {
			log.Printf("DeleteRuleS2SByIDs: RuleS2S %s not found, skipping: %v", id.Key(), err)
			continue
		}

		log.Printf("DeleteRuleS2SByIDs: Found RuleS2S %s with %d saved IEAgAgRuleRefs",
			ruleS2S.Key(), len(ruleS2S.IEAgAgRuleRefs))

		// 1. Сначала пробуем использовать сохраненные ссылки
		if len(ruleS2S.IEAgAgRuleRefs) > 0 {
			log.Printf("DeleteRuleS2SByIDs: Adding %d saved IEAgAgRule references", len(ruleS2S.IEAgAgRuleRefs))
			for _, ref := range ruleS2S.IEAgAgRuleRefs {
				ieAgAgRulesToDelete[ref.Key()] = ref
				log.Printf("DeleteRuleS2SByIDs: Added saved reference: %s", ref.Key())
			}
		} else {
			log.Printf("DeleteRuleS2SByIDs: WARNING - RuleS2S %s has no saved IEAgAgRule references", ruleS2S.Key())
		}

		// 2. ВСЕГДА генерируем ожидаемые правила и проверяем их существование
		log.Printf("DeleteRuleS2SByIDs: Generating expected IEAgAgRules for RuleS2S %s", ruleS2S.Key())
		expectedRules, err := s.GenerateIEAgAgRulesFromRuleS2SWithReader(ctx, reader, *ruleS2S)
		if err != nil {
			log.Printf("DeleteRuleS2SByIDs: ERROR - Failed to generate expected rules for %s: %v", ruleS2S.Key(), err)
			continue
		}

		log.Printf("DeleteRuleS2SByIDs: Generated %d expected IEAgAgRules", len(expectedRules))

		// Проверяем существование каждого ожидаемого правила
		foundCount := 0
		for _, expectedRule := range expectedRules {
			existingRule, err := reader.GetIEAgAgRuleByID(ctx, expectedRule.ResourceIdentifier)
			if err == nil && existingRule != nil {
				ieAgAgRulesToDelete[existingRule.Key()] = existingRule.ResourceIdentifier
				foundCount++
				log.Printf("DeleteRuleS2SByIDs: Found existing IEAgAgRule %s", existingRule.Key())
			} else {
				log.Printf("DeleteRuleS2SByIDs: Expected IEAgAgRule %s not found", expectedRule.Key())
			}
		}

		log.Printf("DeleteRuleS2SByIDs: Found %d out of %d expected IEAgAgRules for RuleS2S %s",
			foundCount, len(expectedRules), ruleS2S.Key())
	}

	// Convert map to slice for deletion
	var ieAgAgRuleIDs []models.ResourceIdentifier
	for _, id := range ieAgAgRulesToDelete {
		ieAgAgRuleIDs = append(ieAgAgRuleIDs, id)
	}

	log.Printf("DeleteRuleS2SByIDs: Total unique IEAgAgRules to delete: %d", len(ieAgAgRuleIDs))

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			log.Printf("DeleteRuleS2SByIDs: Aborting transaction due to error: %v", err)
			writer.Abort()
		}
	}()

	// First delete the associated IEAgAgRules
	if len(ieAgAgRuleIDs) > 0 {
		log.Printf("DeleteRuleS2SByIDs: Deleting %d IEAgAgRules", len(ieAgAgRuleIDs))
		for i, ruleID := range ieAgAgRuleIDs {
			log.Printf("DeleteRuleS2SByIDs: Deleting IEAgAgRule %d/%d: %s", i+1, len(ieAgAgRuleIDs), ruleID.Key())
		}

		if err = writer.DeleteIEAgAgRulesByIDs(ctx, ieAgAgRuleIDs); err != nil {
			log.Printf("DeleteRuleS2SByIDs: ERROR - Failed to delete IEAgAgRules: %v", err)
			return errors.Wrap(err, "failed to delete associated IEAgAgRules")
		}
		log.Printf("DeleteRuleS2SByIDs: Successfully deleted %d IEAgAgRules", len(ieAgAgRuleIDs))
	} else {
		log.Printf("DeleteRuleS2SByIDs: No IEAgAgRules found to delete")
	}

	// Then delete the RuleS2S objects
	log.Printf("DeleteRuleS2SByIDs: Deleting %d RuleS2S objects", len(ids))
	for i, id := range ids {
		log.Printf("DeleteRuleS2SByIDs: Deleting RuleS2S %d/%d: %s", i+1, len(ids), id.Key())
	}

	if err = writer.DeleteRuleS2SByIDs(ctx, ids); err != nil {
		log.Printf("DeleteRuleS2SByIDs: ERROR - Failed to delete RuleS2S objects: %v", err)
		return errors.Wrap(err, "failed to delete rules s2s")
	}

	if err = writer.Commit(); err != nil {
		log.Printf("DeleteRuleS2SByIDs: ERROR - Failed to commit transaction: %v", err)
		return errors.Wrap(err, "failed to commit")
	}

	log.Printf("DeleteRuleS2SByIDs: Successfully deleted %d RuleS2S and %d associated IEAgAgRules",
		len(ids), len(ieAgAgRuleIDs))
	return nil
}

// DeleteServiceAliasesByIDs deletes service aliases by IDs
func (s *NetguardService) DeleteServiceAliasesByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	aliasValidator := validator.GetServiceAliasValidator()

	// Check dependencies for each service alias
	for _, id := range ids {
		if err := aliasValidator.CheckDependencies(ctx, id); err != nil {
			return err
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// Find RuleS2S that depend on these ServiceAliases BEFORE deletion
	affectedRules, err := s.findRuleS2SForServiceAliases(ctx, ids)
	if err != nil {
		return errors.Wrap(err, "failed to find affected RuleS2S")
	}

	log.Printf("DeleteServiceAliasesByIDs: Found %d RuleS2S affected by deletion of %d ServiceAliases", len(affectedRules), len(ids))

	if err = writer.DeleteServiceAliasesByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete service aliases")
	}

	// Update IEAgAgRules for affected RuleS2S (deletion of ServiceAlias makes RuleS2S inactive)
	if len(affectedRules) > 0 {
		// Get reader that can see changes in current transaction
		txReader, err := s.registry.ReaderFromWriter(ctx, writer)
		if err != nil {
			return errors.Wrap(err, "failed to get transaction reader")
		}
		defer txReader.Close()

		// ServiceAlias deletion makes RuleS2S inactive → delete all related IEAgAgRules
		if err = s.updateIEAgAgRulesForRuleS2SWithReader(ctx, writer, txReader, affectedRules, models.SyncOpFullSync); err != nil {
			return errors.Wrap(err, "failed to update IEAgAgRules for affected RuleS2S")
		}

		log.Printf("DeleteServiceAliasesByIDs: Updated IEAgAgRules for %d affected RuleS2S", len(affectedRules))
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	return nil
}

// syncAddressGroupBindingPolicies синхронизирует политики привязки групп адресов с указанной операцией
func (s *NetguardService) syncAddressGroupBindingPolicies(ctx context.Context, writer ports.Writer, policies []models.AddressGroupBindingPolicy, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

		validator := validation.NewDependencyValidator(reader)
		policyValidator := validator.GetAddressGroupBindingPolicyValidator()

		for i := range policies {
			// Используем указатель на элемент слайса, чтобы изменения сохранились
			policy := &policies[i]

			existingPolicy, err := reader.GetAddressGroupBindingPolicyByID(ctx, policy.ResourceIdentifier)
			if err == nil && syncOp != models.SyncOpDelete {
				// Политика существует - используем ValidateForUpdate
				if err := policyValidator.ValidateForUpdate(ctx, *existingPolicy, policy); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound && syncOp != models.SyncOpDelete {
				// Политика новая - используем ValidateForCreation
				if err := policyValidator.ValidateForCreation(ctx, policy); err != nil {
					return err
				}
			} else if err != nil && err != ports.ErrNotFound {
				// Произошла другая ошибка
				return errors.Wrap(err, "failed to get address group binding policy")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if len(policies) > 0 {
		var ids []models.ResourceIdentifier
		for _, policy := range policies {
			ids = append(ids, policy.ResourceIdentifier)
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Выполнение операции с указанной опцией
	if err := writer.SyncAddressGroupBindingPolicies(ctx, policies, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync address group binding policies")
	}

	if err := writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	return nil
}

// SyncAddressGroupBindingPolicies syncs address group binding policies
func (s *NetguardService) SyncAddressGroupBindingPolicies(ctx context.Context, policies []models.AddressGroupBindingPolicy, scope ports.Scope) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	policyValidator := validator.GetAddressGroupBindingPolicyValidator()

	// Validate all policies
	for i := range policies {
		// Используем указатель на элемент слайса, чтобы изменения сохранились
		policy := &policies[i]

		// Check if policy exists
		existingPolicy, err := reader.GetAddressGroupBindingPolicyByID(ctx, policy.ResourceIdentifier)
		if err == nil {
			// Policy exists - use ValidateForUpdate
			if err := policyValidator.ValidateForUpdate(ctx, *existingPolicy, policy); err != nil {
				return err
			}
		} else if err == ports.ErrNotFound {
			// Policy is new - use ValidateForCreation
			if err := policyValidator.ValidateForCreation(ctx, policy); err != nil {
				return err
			}
		} else {
			// Other error occurred
			return errors.Wrap(err, "failed to get address group binding policy")
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncAddressGroupBindingPolicies(ctx, policies, scope, ports.WithSyncOp(models.SyncOpFullSync)); err != nil {
		return errors.Wrap(err, "failed to sync address group binding policies")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	for i := range policies {
		s.conditionManager.ProcessAddressGroupBindingPolicyConditions(ctx, &policies[i])
		if err := s.conditionManager.saveResourceConditions(ctx, &policies[i]); err != nil {
			log.Printf("Failed to save address group binding policy conditions for %s: %v", policies[i].Key(), err)
		}
	}
	return nil
}

// GetIEAgAgRules returns a list of IEAgAgRules
func (s *NetguardService) GetIEAgAgRules(ctx context.Context, scope ports.Scope) ([]models.IEAgAgRule, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var result []models.IEAgAgRule

	err = reader.ListIEAgAgRules(ctx, func(rule models.IEAgAgRule) error {
		result = append(result, rule)
		return nil
	}, scope)

	if err != nil {
		return nil, errors.Wrap(err, "failed to list IEAgAgRules")
	}

	return result, nil
}

// GetIEAgAgRuleByID returns a IEAgAgRule by ID
func (s *NetguardService) GetIEAgAgRuleByID(ctx context.Context, id models.ResourceIdentifier) (*models.IEAgAgRule, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	return reader.GetIEAgAgRuleByID(ctx, id)
}

// syncIEAgAgRulesWithReader синхронизирует правила IEAgAgRule с указанной операцией, используя переданный reader
func (s *NetguardService) syncIEAgAgRulesWithReader(ctx context.Context, writer ports.Writer, reader ports.Reader, rules []models.IEAgAgRule, syncOp models.SyncOp) error {
	log.Printf("🚀 CALLED: syncIEAgAgRulesWithReader function called with %d rules, syncOp=%s", len(rules), syncOp)
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {

		validator := validation.NewDependencyValidator(reader)
		ruleValidator := validator.GetIEAgAgRuleValidator()

		for _, rule := range rules {
			existingRule, err := reader.GetIEAgAgRuleByID(ctx, rule.ResourceIdentifier)
			if err == nil && syncOp != models.SyncOpDelete {
				// Правило существует - используем ValidateForUpdate
				if err := ruleValidator.ValidateForUpdate(ctx, *existingRule, rule); err != nil {
					return err
				}
			} else if syncOp != models.SyncOpDelete {
				// Правило новое - используем ValidateForCreation
				if err := ruleValidator.ValidateForCreation(ctx, rule); err != nil {
					return err
				}
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if len(rules) > 0 {
		var ids []models.ResourceIdentifier
		for _, rule := range rules {
			ids = append(ids, rule.ResourceIdentifier)
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Выполнение операции с указанной опцией
	if err := writer.SyncIEAgAgRules(ctx, rules, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync IEAgAgRules")
	}

	if err := writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Sync with sgroups after successful commit
	if s.syncManager == nil {
		log.Printf("⚠️  WARNING: syncManager is nil, skipping sgroups synchronization for %d IEAgAgRule(s)", len(rules))
	} else if len(rules) == 0 {
		log.Printf("⚠️  WARNING: No IEAgAgRules to sync with sgroups")
	} else {
		log.Printf("🔧 DEBUG: Starting sgroups synchronization for %d IEAgAgRule(s)", len(rules))
		for i := range rules {
			log.Printf("🔧 DEBUG: Syncing IEAgAgRule %s with sgroups (Transport: %s, Traffic: %s, Action: %s)",
				rules[i].Key(), rules[i].Transport, rules[i].Traffic, rules[i].Action)

			if syncOp == models.SyncOpDelete {
				if syncErr := s.syncManager.SyncEntity(ctx, &rules[i], types.SyncOperationDelete); syncErr != nil {
					log.Printf("❌ Failed to sync IEAgAgRule %s with sgroups (delete): %v", rules[i].Key(), syncErr)
				} else {
					log.Printf("✅ Successfully synced IEAgAgRule %s with sgroups (delete)", rules[i].Key())
				}
			} else {
				if syncErr := s.syncManager.SyncEntity(ctx, &rules[i], types.SyncOperationUpsert); syncErr != nil {
					log.Printf("❌ Failed to sync IEAgAgRule %s with sgroups: %v", rules[i].Key(), syncErr)
				} else {
					log.Printf("✅ Successfully synced IEAgAgRule %s with sgroups", rules[i].Key())
				}
			}
		}
		log.Printf("✅ DEBUG: Completed sgroups synchronization for IEAgAgRules")
	}

	// Process conditions after sgroups sync
	for i := range rules {
		s.conditionManager.ProcessIEAgAgRuleConditions(ctx, &rules[i])
		if err := s.conditionManager.saveResourceConditions(ctx, &rules[i]); err != nil {
			log.Printf("Failed to save IEAgAgRule conditions for %s: %v", rules[i].Key(), err)
		}
	}
	return nil
}

// syncIEAgAgRules синхронизирует правила IEAgAgRule с указанной операцией (создает новый reader)
func (s *NetguardService) syncIEAgAgRules(ctx context.Context, writer ports.Writer, rules []models.IEAgAgRule, syncOp models.SyncOp) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	return s.syncIEAgAgRulesWithReader(ctx, writer, reader, rules, syncOp)
}

// syncNetworks синхронизирует сети с указанной операцией
func (s *NetguardService) syncNetworks(ctx context.Context, writer ports.Writer, networks []models.Network, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

		// Create validator
		validator := validation.NewDependencyValidator(reader)
		networkValidator := validator.GetNetworkValidator()

		// Validate all networks
		for i := range networks {
			network := &networks[i]

			// Check if network exists
			networkID := models.ResourceIdentifier{Name: network.Name, Namespace: network.Namespace}
			existingNetwork, err := reader.GetNetworkByID(ctx, networkID)
			if err == nil {
				// Network exists - use ValidateForUpdate
				if err := networkValidator.ValidateForUpdate(ctx, *existingNetwork, *network); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound {
				// Network is new - use ValidateForCreation
				if err := networkValidator.ValidateForCreation(ctx, *network); err != nil {
					return err
				}
			} else {
				// Other error occurred
				return errors.Wrap(err, "failed to get network")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if len(networks) > 0 {
		var ids []models.ResourceIdentifier
		for _, network := range networks {
			ids = append(ids, models.ResourceIdentifier{Name: network.Name, Namespace: network.Namespace})
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	// Если это удаление, используем DeleteNetworksByIDs для корректного удаления
	if syncOp == models.SyncOpDelete {
		// Собираем ID сетей
		var ids []models.ResourceIdentifier
		for _, network := range networks {
			ids = append(ids, models.ResourceIdentifier{Name: network.Name, Namespace: network.Namespace})
		}

		// Используем DeleteNetworksByIDs для удаления сетей
		return s.DeleteNetworksByIDs(ctx, ids)
	}

	// Выполнение операции с указанной опцией для не-удаления
	if err := writer.SyncNetworks(ctx, networks, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync networks")
	}

	if err := writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Синхронизация с sgroups после успешного commit'а (только для операций создания/обновления)
	var sgroupsSyncResults map[string]error
	if syncOp != models.SyncOpDelete {
		log.Printf("🔧 DEBUG: syncNetworks - Starting sgroups synchronization for %d Networks", len(networks))
		sgroupsSyncResults = s.syncNetworksWithSGroups(ctx, networks, types.SyncOperationUpsert)
		log.Printf("✅ DEBUG: syncNetworks - Completed sgroups synchronization")
	} else {
		log.Printf("⚠️  DEBUG: syncNetworks - Skipping sgroups sync for DELETE operation (handled separately)")
		// Для DELETE операций считаем что синхронизация успешна (нет необходимости в sgroups sync)
		sgroupsSyncResults = make(map[string]error)
		for _, network := range networks {
			sgroupsSyncResults[network.GetSyncKey()] = nil
		}
	}

	for i := range networks {
		network := &networks[i]
		syncResult := sgroupsSyncResults[network.GetSyncKey()]
		s.conditionManager.ProcessNetworkConditions(ctx, network, syncResult)
		if err := s.conditionManager.saveResourceConditions(ctx, network); err != nil {
			log.Printf("Failed to save network conditions for %s: %v", network.Key(), err)
		}
	}
	return nil
}

// syncNetworkBindings синхронизирует привязки сетей с указанной операцией
func (s *NetguardService) syncNetworkBindings(ctx context.Context, writer ports.Writer, bindings []models.NetworkBinding, syncOp models.SyncOp) error {
	// Валидация в зависимости от операции
	if syncOp != models.SyncOpDelete {
		reader, err := s.registry.Reader(ctx)
		if err != nil {
			return errors.Wrap(err, "failed to get reader")
		}
		defer reader.Close()

		// Create validator
		validator := validation.NewDependencyValidator(reader)
		bindingValidator := validator.GetNetworkBindingValidator()

		// Validate all bindings
		for i := range bindings {
			binding := &bindings[i]

			// Check if binding exists
			bindingID := models.ResourceIdentifier{Name: binding.Name, Namespace: binding.Namespace}
			existingBinding, err := reader.GetNetworkBindingByID(ctx, bindingID)
			if err == nil {
				// Binding exists - use ValidateForUpdate
				if err := bindingValidator.ValidateForUpdate(ctx, *existingBinding, *binding); err != nil {
					return err
				}
			} else if err == ports.ErrNotFound {
				// Binding is new - use ValidateForCreation
				if err := bindingValidator.ValidateForCreation(ctx, *binding); err != nil {
					return err
				}
			} else {
				// Other error occurred
				return errors.Wrap(err, "failed to get network binding")
			}
		}
	}

	// Определение scope
	var scope ports.Scope
	if len(bindings) > 0 {
		var ids []models.ResourceIdentifier
		for _, binding := range bindings {
			ids = append(ids, models.ResourceIdentifier{Name: binding.Name, Namespace: binding.Namespace})
		}
		scope = ports.NewResourceIdentifierScope(ids...)
	} else {
		scope = ports.EmptyScope{}
	}

	log.Printf("🔄 DEBUG: syncNetworkBindings - Starting to sync %d NetworkBindings with operation: %s", len(bindings), syncOp)

	// 1. Сначала сохраняем NetworkBindings в транзакции
	if err := writer.SyncNetworkBindings(ctx, bindings, scope, ports.WithSyncOp(syncOp)); err != nil {
		return errors.Wrap(err, "failed to sync network bindings")
	}
	log.Printf("✅ DEBUG: syncNetworkBindings - Successfully synced NetworkBindings to transaction")

	// 2. Получаем reader который видит незакоммиченные изменения в текущей транзакции
	txReader, err := s.registry.ReaderFromWriter(ctx, writer)
	if err != nil {
		return errors.Wrap(err, "failed to get transaction reader")
	}
	defer txReader.Close()
	log.Printf("✅ DEBUG: syncNetworkBindings - Got transaction reader")

	// 3. В ТОЙ ЖЕ транзакции обновляем связанные Network и AddressGroup
	var networksToSync []models.Network
	var addressGroupsToSync []models.AddressGroup

	if syncOp != models.SyncOpDelete {
		log.Printf("🔄 DEBUG: syncNetworkBindings - Processing %d bindings for Network and AddressGroup updates IN SAME TRANSACTION", len(bindings))

		for _, binding := range bindings {
			log.Printf("🔄 DEBUG: syncNetworkBindings - Processing binding %s", binding.Key())

			// Обновляем Network в той же транзакции
			networkRef := models.ResourceIdentifier{Name: binding.NetworkRef.Name, Namespace: binding.Namespace}
			log.Printf("🔄 DEBUG: syncNetworkBindings - Getting Network %s", networkRef.Key())

			network, err := txReader.GetNetworkByID(ctx, networkRef)
			if err != nil {
				return errors.Wrapf(err, "failed to get network %s", networkRef.Key())
			}
			log.Printf("✅ DEBUG: syncNetworkBindings - Found Network %s, updating binding references", network.Key())

			// Обновляем Network с binding references
			network.BindingRef = &v1beta1.ObjectReference{
				APIVersion: "netguard.sgroups.io/v1beta1",
				Kind:       "NetworkBinding",
				Name:       binding.Name,
			}
			network.AddressGroupRef = &v1beta1.ObjectReference{
				APIVersion: "netguard.sgroups.io/v1beta1",
				Kind:       "AddressGroup",
				Name:       binding.AddressGroupRef.Name,
			}
			network.IsBound = true
			network.Meta.TouchOnWrite(fmt.Sprintf("binding-%d", time.Now().UnixNano()))

			// Сохраняем обновленный Network в той же транзакции
			log.Printf("🔄 DEBUG: syncNetworkBindings - About to call writer.SyncNetworks for %s", network.Key())
			log.Printf("🔄 DEBUG: syncNetworkBindings - Network details: IsBound=%t, BindingRef=%v, AddressGroupRef=%v",
				network.IsBound, network.BindingRef, network.AddressGroupRef)
			if err := writer.SyncNetworks(ctx, []models.Network{*network}, ports.NewResourceIdentifierScope(networkRef)); err != nil {
				return errors.Wrapf(err, "failed to sync network %s in transaction", networkRef.Key())
			}
			networksToSync = append(networksToSync, *network)
			log.Printf("✅ DEBUG: syncNetworkBindings - Updated Network %s in transaction (should be in writer now)", network.Key())

			// Обновляем AddressGroup в той же транзакции
			addressGroupRef := models.ResourceIdentifier{Name: binding.AddressGroupRef.Name, Namespace: binding.Namespace}
			log.Printf("🔄 DEBUG: syncNetworkBindings - Getting AddressGroup %s", addressGroupRef.Key())

			addressGroup, err := txReader.GetAddressGroupByID(ctx, addressGroupRef)
			if err != nil {
				return errors.Wrapf(err, "failed to get address group %s", addressGroupRef.Key())
			}
			log.Printf("✅ DEBUG: syncNetworkBindings - Found AddressGroup %s, checking if Network already in Networks list", addressGroup.Key())

			// Проверяем, есть ли уже Network в AddressGroup
			networkName := fmt.Sprintf("%s/%s", network.Namespace, network.Name)
			networkExists := false
			for _, item := range addressGroup.Networks {
				if item.Name == networkName {
					networkExists = true
					break
				}
			}

			if !networkExists {
				log.Printf("🔄 DEBUG: syncNetworkBindings - Adding Network %s to AddressGroup %s Networks list", networkName, addressGroup.Key())

				// Добавляем Network в AddressGroup
				networkItem := models.NetworkItem{
					Name:       networkName,
					CIDR:       network.CIDR,
					ApiVersion: "netguard.sgroups.io/v1beta1",
					Kind:       "Network",
					Namespace:  network.Namespace,
				}
				addressGroup.Networks = append(addressGroup.Networks, networkItem)
				addressGroup.Meta.TouchOnWrite(fmt.Sprintf("binding-%d", time.Now().UnixNano()))

				// Сохраняем обновленный AddressGroup в той же транзакции
				log.Printf("🔄 DEBUG: syncNetworkBindings - About to call writer.SyncAddressGroups for %s", addressGroup.Key())
				log.Printf("🔄 DEBUG: syncNetworkBindings - AddressGroup Networks count: %d", len(addressGroup.Networks))
				if err := writer.SyncAddressGroups(ctx, []models.AddressGroup{*addressGroup}, ports.NewResourceIdentifierScope(addressGroupRef)); err != nil {
					return errors.Wrapf(err, "failed to sync address group %s in transaction", addressGroupRef.Key())
				}
				addressGroupsToSync = append(addressGroupsToSync, *addressGroup)
				log.Printf("✅ DEBUG: syncNetworkBindings - Updated AddressGroup %s in transaction (should be in writer now)", addressGroup.Key())
			} else {
				log.Printf("ℹ️  DEBUG: syncNetworkBindings - Network %s already exists in AddressGroup %s", networkName, addressGroup.Key())
				addressGroupsToSync = append(addressGroupsToSync, *addressGroup)
			}
		}
	} else {
		log.Printf("🔄 DEBUG: syncNetworkBindings - Processing %d bindings for deletion IN SAME TRANSACTION", len(bindings))

		for _, binding := range bindings {
			log.Printf("🔄 DEBUG: syncNetworkBindings - Processing deletion for binding %s", binding.Key())

			// Очищаем references в Network
			networkRef := models.ResourceIdentifier{Name: binding.NetworkRef.Name, Namespace: binding.Namespace}
			network, err := txReader.GetNetworkByID(ctx, networkRef)
			if err != nil {
				if err == ports.ErrNotFound {
					log.Printf("⚠️  DEBUG: syncNetworkBindings - Network %s not found for deletion, skipping", networkRef.Key())
				} else {
					return errors.Wrapf(err, "failed to get network %s for deletion", networkRef.Key())
				}
			} else {
				log.Printf("🔄 DEBUG: syncNetworkBindings - Clearing binding references in Network %s", network.Key())

				// Очищаем binding references
				network.BindingRef = nil
				network.AddressGroupRef = nil
				network.IsBound = false
				network.Meta.TouchOnWrite(fmt.Sprintf("unbinding-%d", time.Now().UnixNano()))

				if err := writer.SyncNetworks(ctx, []models.Network{*network}, ports.NewResourceIdentifierScope(networkRef)); err != nil {
					return errors.Wrapf(err, "failed to sync network %s for deletion", networkRef.Key())
				}
				networksToSync = append(networksToSync, *network)
				log.Printf("✅ DEBUG: syncNetworkBindings - Cleared Network %s references in transaction", network.Key())
			}

			// Удаляем Network из AddressGroup
			addressGroupRef := models.ResourceIdentifier{Name: binding.AddressGroupRef.Name, Namespace: binding.Namespace}
			addressGroup, err := txReader.GetAddressGroupByID(ctx, addressGroupRef)
			if err != nil {
				if err == ports.ErrNotFound {
					log.Printf("⚠️  DEBUG: syncNetworkBindings - AddressGroup %s not found for deletion, skipping", addressGroupRef.Key())
				} else {
					return errors.Wrapf(err, "failed to get address group %s for deletion", addressGroupRef.Key())
				}
			} else {
				log.Printf("🔄 DEBUG: syncNetworkBindings - Removing Network from AddressGroup %s", addressGroup.Key())

				// Удаляем Network из AddressGroup (используем правильный формат namespace/name)
				networkName := fmt.Sprintf("%s/%s", binding.Namespace, binding.NetworkRef.Name)
				var updatedNetworks []models.NetworkItem
				removedCount := 0
				for _, item := range addressGroup.Networks {
					if item.Name != networkName {
						updatedNetworks = append(updatedNetworks, item)
					} else {
						removedCount++
					}
				}

				if removedCount > 0 {
					addressGroup.Networks = updatedNetworks
					addressGroup.Meta.TouchOnWrite(fmt.Sprintf("unbinding-%d", time.Now().UnixNano()))

					if err := writer.SyncAddressGroups(ctx, []models.AddressGroup{*addressGroup}, ports.NewResourceIdentifierScope(addressGroupRef)); err != nil {
						return errors.Wrapf(err, "failed to sync address group %s for deletion", addressGroupRef.Key())
					}
					addressGroupsToSync = append(addressGroupsToSync, *addressGroup)
					log.Printf("✅ DEBUG: syncNetworkBindings - Removed Network %s from AddressGroup %s in transaction", networkName, addressGroup.Key())
				} else {
					log.Printf("ℹ️  DEBUG: syncNetworkBindings - Network %s not found in AddressGroup %s", networkName, addressGroup.Key())
					addressGroupsToSync = append(addressGroupsToSync, *addressGroup)
				}
			}
		}
	}

	// 4. Коммитим ВСЕ изменения разом (NetworkBindings + Networks + AddressGroups)
	log.Printf("🔄 DEBUG: syncNetworkBindings - About to commit transaction")
	log.Printf("🔄 DEBUG: syncNetworkBindings - Expected to commit %d Networks and %d AddressGroups", len(networksToSync), len(addressGroupsToSync))
	if err := writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit all changes")
	}
	log.Printf("✅ DEBUG: syncNetworkBindings - Successfully committed all changes")

	// 5. После успешного коммита синхронизируем с SGROUP
	if len(networksToSync) > 0 {
		log.Printf("🔄 DEBUG: syncNetworkBindings - Syncing %d Networks with SGROUP (FORCED)", len(networksToSync))
		networkSyncResults := s.syncNetworksWithSGroupsForced(ctx, networksToSync, types.SyncOperationUpsert)
		log.Printf("✅ DEBUG: syncNetworkBindings - Completed Networks sync with SGROUP")

		// Логируем результаты синхронизации для отладки
		for _, network := range networksToSync {
			if err := networkSyncResults[network.GetSyncKey()]; err != nil {
				log.Printf("❌ ERROR: syncNetworkBindings - Network %s sync failed: %v", network.GetSyncKey(), err)
			}
		}
	}

	if len(addressGroupsToSync) > 0 {
		log.Printf("🔄 DEBUG: syncNetworkBindings - Syncing %d AddressGroups with SGROUP (FORCED)", len(addressGroupsToSync))
		s.syncAddressGroupsWithSGroupsForced(ctx, addressGroupsToSync, types.SyncOperationUpsert)
		log.Printf("✅ DEBUG: syncNetworkBindings - Completed AddressGroups sync with SGROUP")
	}

	// Используем универсальную функцию для обработки conditions
	s.processConditionsIfNeeded(ctx, bindings, syncOp)
	return nil
}

// GenerateIEAgAgRulesFromRuleS2S генерирует правила IEAgAgRule на основе RuleS2S
func (s *NetguardService) GenerateIEAgAgRulesFromRuleS2S(ctx context.Context, ruleS2S models.RuleS2S) ([]models.IEAgAgRule, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	return s.GenerateIEAgAgRulesFromRuleS2SWithReader(ctx, reader, ruleS2S)
}

// GenerateIEAgAgRulesFromRuleS2SWithReader генерирует правила IEAgAgRule на основе RuleS2S, используя переданный reader
func (s *NetguardService) GenerateIEAgAgRulesFromRuleS2SWithReader(ctx context.Context, reader ports.Reader, ruleS2S models.RuleS2S) ([]models.IEAgAgRule, error) {
	klog.Infof("🔥 DEBUG: GenerateIEAgAgRulesFromRuleS2SWithReader called for RuleS2S: %s (traffic: %s)", ruleS2S.Key(), ruleS2S.Traffic)
	klog.Infof("🔥 DEBUG: RuleS2S references - local: %s, target: %s", ruleS2S.ServiceLocalRef.Key(), ruleS2S.ServiceRef.Key())

	// Получаем сервисы по ссылкам
	localServiceAlias, err := reader.GetServiceAliasByID(ctx, ruleS2S.ServiceLocalRef.ResourceIdentifier)
	if err != nil {
		return nil, errors.Wrapf(err, "failed to get local service alias %s", ruleS2S.ServiceLocalRef.Key())
	}

	targetServiceAlias, err := reader.GetServiceAliasByID(ctx, ruleS2S.ServiceRef.ResourceIdentifier)
	if err != nil {
		return nil, errors.Wrapf(err, "failed to get target service alias %s", ruleS2S.ServiceRef.Key())
	}

	localService, err := reader.GetServiceByID(ctx, localServiceAlias.ServiceRef.ResourceIdentifier)
	if err != nil {
		return nil, errors.Wrapf(err, "failed to get local service %s", localServiceAlias.ServiceRef.Key())
	}
	klog.Infof("🔥 DEBUG: Retrieved localService %s from DB: %d IngressPorts", localService.Key(), len(localService.IngressPorts))
	for i, port := range localService.IngressPorts {
		klog.Infof("🔥 DEBUG: localService.IngressPorts[%d]: %s/%s", i, port.Port, port.Protocol)
	}

	targetService, err := reader.GetServiceByID(ctx, targetServiceAlias.ServiceRef.ResourceIdentifier)
	if err != nil {
		return nil, errors.Wrapf(err, "failed to get target service %s", targetServiceAlias.ServiceRef.Key())
	}
	klog.Infof("🔥 DEBUG: Retrieved targetService %s from DB: %d IngressPorts", targetService.Key(), len(targetService.IngressPorts))
	for i, port := range targetService.IngressPorts {
		klog.Infof("🔥 DEBUG: targetService.IngressPorts[%d]: %s/%s", i, port.Port, port.Protocol)
	}

	// Получаем группы адресов
	localAddressGroups := localService.AddressGroups
	targetAddressGroups := targetService.AddressGroups

	// Определяем порты в зависимости от направления трафика
	klog.Infof("🔥 DEBUG: RuleS2S %s - localService: %s (ports: %d), targetService: %s (ports: %d)",
		ruleS2S.Key(), localService.Key(), len(localService.IngressPorts), targetService.Key(), len(targetService.IngressPorts))

	for i, port := range localService.IngressPorts {
		klog.Infof("🔥 DEBUG: localService.IngressPorts[%d]: %s/%s", i, port.Port, port.Protocol)
	}
	for i, port := range targetService.IngressPorts {
		klog.Infof("🔥 DEBUG: targetService.IngressPorts[%d]: %s/%s", i, port.Port, port.Protocol)
	}

	var ports []models.IngressPort
	if ruleS2S.Traffic == models.INGRESS {
		ports = localService.IngressPorts
		klog.Infof("🔥 DEBUG: Using localService ports for INGRESS traffic: %d ports", len(ports))
	} else {
		ports = targetService.IngressPorts
		klog.Infof("🔥 DEBUG: Using targetService ports for EGRESS traffic: %d ports", len(ports))
	}

	klog.Infof("🔥 DEBUG: Final ports array: %v", ports)

	// Создаем правила IEAgAgRule
	var result []models.IEAgAgRule

	// Группируем порты по протоколу
	tcpPorts := []string{}
	udpPorts := []string{}

	for _, port := range ports {
		if port.Protocol == models.TCP {
			tcpPorts = append(tcpPorts, port.Port)
		} else if port.Protocol == models.UDP {
			udpPorts = append(udpPorts, port.Port)
		}
	}

	// Создаем правила для каждой комбинации групп адресов и протоколов
	for _, localAG := range localAddressGroups {
		for _, targetAG := range targetAddressGroups {
			// Создаем TCP правило
			if len(tcpPorts) > 0 {
				tcpRule := models.IEAgAgRule{
					SelfRef: models.SelfRef{
						ResourceIdentifier: models.NewResourceIdentifier(
							generateRuleName(string(ruleS2S.Traffic), localAG.Name, targetAG.Name, string(models.TCP)),
							models.WithNamespace(determineRuleNamespace(ruleS2S, localAG, targetAG)),
						),
					},
					Transport:         models.TCP,
					Traffic:           ruleS2S.Traffic,
					AddressGroupLocal: localAG,
					AddressGroup:      targetAG,
					Ports: []models.PortSpec{
						{
							Destination: strings.Join(tcpPorts, ","),
						},
					},
					Action:   models.ActionAccept,
					Logs:     true,
					Trace:    ruleS2S.Trace, // Пробрасываем trace из RuleS2S
					Priority: 100,
				}
				result = append(result, tcpRule)
			}

			// Создаем UDP правило
			if len(udpPorts) > 0 {
				udpRule := models.IEAgAgRule{
					SelfRef: models.SelfRef{
						ResourceIdentifier: models.NewResourceIdentifier(
							generateRuleName(string(ruleS2S.Traffic), localAG.Name, targetAG.Name, string(models.UDP)),
							models.WithNamespace(determineRuleNamespace(ruleS2S, localAG, targetAG)),
						),
					},
					Transport:         models.UDP,
					Traffic:           ruleS2S.Traffic,
					AddressGroupLocal: localAG,
					AddressGroup:      targetAG,
					Ports: []models.PortSpec{
						{
							Destination: strings.Join(udpPorts, ","),
						},
					},
					Action:   models.ActionAccept,
					Logs:     true,
					Trace:    ruleS2S.Trace, // Пробрасываем trace из RuleS2S
					Priority: 100,
				}
				result = append(result, udpRule)
			}
		}
	}

	return result, nil
}

// generateRuleName создает детерминированное имя правила
func generateRuleName(trafficDirection, localAGName, targetAGName, protocol string) string {
	input := fmt.Sprintf("%s-%s-%s-%s",
		strings.ToLower(trafficDirection),
		localAGName,
		targetAGName,
		strings.ToLower(protocol))

	h := sha256.New()
	h.Write([]byte(input))
	hash := h.Sum(nil)

	// Форматируем первые 16 байт как UUID v5
	uuid := fmt.Sprintf("%x-%x-%x-%x-%x",
		hash[0:4], hash[4:6], hash[6:8], hash[8:10], hash[10:16])

	// Используем префикс направления трафика и UUID
	return fmt.Sprintf("%s-%s",
		strings.ToLower(trafficDirection)[:3],
		uuid)
}

// determineRuleNamespace определяет пространство имен для правила
func determineRuleNamespace(ruleS2S models.RuleS2S, localAG, targetAG models.AddressGroupRef) string {
	if ruleS2S.Traffic == models.INGRESS {
		// Для входящего трафика правило размещается в пространстве имен локальной группы адресов
		if localAG.Namespace != "" {
			return localAG.Namespace
		}
		return ruleS2S.Namespace
	} else {
		// Для исходящего трафика правило размещается в пространстве имен целевой группы адресов
		if targetAG.Namespace != "" {
			return targetAG.Namespace
		}
		return ruleS2S.Namespace
	}
}

// SyncIEAgAgRules синхронизирует правила IEAgAgRule
func (s *NetguardService) SyncIEAgAgRules(ctx context.Context, rules []models.IEAgAgRule, scope ports.Scope) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewDependencyValidator(reader)
	ruleValidator := validator.GetIEAgAgRuleValidator()

	// Validate all rules
	for _, rule := range rules {
		// Check if rule exists
		existingRule, err := reader.GetIEAgAgRuleByID(ctx, rule.ResourceIdentifier)
		if err == nil {
			// Rule exists - use ValidateForUpdate
			if err := ruleValidator.ValidateForUpdate(ctx, *existingRule, rule); err != nil {
				return err
			}
		} else {
			// Rule is new - use ValidateForCreation
			if err := ruleValidator.ValidateForCreation(ctx, rule); err != nil {
				return err
			}
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = s.syncIEAgAgRules(ctx, writer, rules, models.SyncOpFullSync); err != nil {
		return errors.Wrap(err, "failed to sync IEAgAgRules")
	}
	return nil
}

// SyncNetworks синхронизирует сети
func (s *NetguardService) SyncNetworks(ctx context.Context, networks []models.Network, scope ports.Scope) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator for Network validation
	validator := validation.NewDependencyValidator(reader)
	networkValidator := validator.GetNetworkValidator()

	// Validate all networks
	for i := range networks {
		network := &networks[i]

		// Check if network exists
		networkID := models.ResourceIdentifier{Name: network.Name, Namespace: network.Namespace}
		existingNetwork, err := reader.GetNetworkByID(ctx, networkID)
		if err == nil {
			// Network exists - use ValidateForUpdate
			if err := networkValidator.ValidateForUpdate(ctx, *existingNetwork, *network); err != nil {
				return err
			}
		} else if err == ports.ErrNotFound {
			// Network is new - use ValidateForCreation
			if err := networkValidator.ValidateForCreation(ctx, *network); err != nil {
				return err
			}
		} else {
			// Other error occurred
			return errors.Wrap(err, "failed to get network")
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.SyncNetworks(ctx, networks, scope, ports.WithSyncOp(models.SyncOpFullSync)); err != nil {
		return errors.Wrap(err, "failed to sync networks")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Синхронизация с sgroups после успешного commit'а
	log.Printf("🔧 DEBUG: SyncNetworks - Starting sgroups synchronization for %d Networks", len(networks))
	sgroupsSyncResults := s.syncNetworksWithSGroups(ctx, networks, types.SyncOperationUpsert)
	log.Printf("✅ DEBUG: SyncNetworks - Completed sgroups synchronization")

	for i := range networks {
		network := &networks[i]
		syncResult := sgroupsSyncResults[network.GetSyncKey()]
		s.conditionManager.ProcessNetworkConditions(ctx, network, syncResult)
		if err := s.conditionManager.saveResourceConditions(ctx, network); err != nil {
			log.Printf("Failed to save network conditions for %s: %v", network.Key(), err)
		}
	}
	return nil
}

// SyncNetworkBindings синхронизирует привязки сетей
func (s *NetguardService) SyncNetworkBindings(ctx context.Context, bindings []models.NetworkBinding, scope ports.Scope) error {
	log.Printf("🚨 DEBUG: SyncNetworkBindings PUBLIC METHOD CALLED with %d bindings", len(bindings))
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator for NetworkBinding validation
	validator := validation.NewDependencyValidator(reader)
	bindingValidator := validator.GetNetworkBindingValidator()

	// Validate all bindings
	for i := range bindings {
		binding := &bindings[i]

		// Check if binding exists
		bindingID := models.ResourceIdentifier{Name: binding.Name, Namespace: binding.Namespace}
		existingBinding, err := reader.GetNetworkBindingByID(ctx, bindingID)
		if err == nil {
			// Binding exists - use ValidateForUpdate
			if err := bindingValidator.ValidateForUpdate(ctx, *existingBinding, *binding); err != nil {
				return err
			}
		} else if err == ports.ErrNotFound {
			// Binding is new - use ValidateForCreation
			if err := bindingValidator.ValidateForCreation(ctx, *binding); err != nil {
				return err
			}
		} else {
			// Other error occurred
			return errors.Wrap(err, "failed to get network binding")
		}
	}

	// Use the same logic as the general Sync method for consistency
	// This ensures atomic updates of NetworkBinding, Network, and AddressGroup
	log.Printf("🔄 DEBUG: SyncNetworkBindings - Using unified sync logic for %d NetworkBindings", len(bindings))

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	// Use the internal syncNetworkBindings method that handles atomicity
	if err := s.syncNetworkBindings(ctx, writer, bindings, models.SyncOpFullSync); err != nil {
		return err
	}

	// NOTE: We do NOT call processConditionsIfNeeded here because syncNetworkBindings
	// already handles condition processing within the same transaction.
	// Calling it here would create a separate commit that overwrites our atomic changes.

	log.Printf("✅ DEBUG: SyncNetworkBindings - Successfully completed unified sync for %d NetworkBindings", len(bindings))
	return nil
}

// DeleteIEAgAgRulesByIDs deletes IEAgAgRules by IDs
func (s *NetguardService) DeleteIEAgAgRulesByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.DeleteIEAgAgRulesByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete IEAgAgRules")
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	return nil
}

// GetIEAgAgRulesByIDs returns a list of IEAgAgRules by IDs
func (s *NetguardService) GetIEAgAgRulesByIDs(ctx context.Context, ids []models.ResourceIdentifier) ([]models.IEAgAgRule, error) {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return nil, errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	var result []models.IEAgAgRule

	for _, id := range ids {
		rule, err := reader.GetIEAgAgRuleByID(ctx, id)
		if err != nil {
			return nil, errors.Wrapf(err, "failed to get IEAgAgRule %s", id.Key())
		}
		if rule != nil {
			result = append(result, *rule)
		}
	}

	return result, nil
}

// DeleteAddressGroupBindingPoliciesByIDs deletes address group binding policies by IDs
func (s *NetguardService) DeleteAddressGroupBindingPoliciesByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator
	validator := validation.NewAddressGroupBindingPolicyValidator(reader)

	// Check dependencies for each policy
	for _, id := range ids {
		if err := validator.CheckDependencies(ctx, id); err != nil {
			return err
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.DeleteAddressGroupBindingPoliciesByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete address group binding policies")
	}
	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}
	return nil
}

// DeleteNetworksByIDs deletes networks by IDs
func (s *NetguardService) DeleteNetworksByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Get networks that will be deleted (for syncing with SGROUP after deletion)
	var networks []models.Network
	for _, id := range ids {
		network, err := reader.GetNetworkByID(ctx, id)
		if err != nil {
			continue // Skip if network doesn't exist
		}
		networks = append(networks, *network)
	}

	// Create validator for dependency checking
	validator := validation.NewDependencyValidator(reader)
	networkValidator := validator.GetNetworkValidator()

	// Check dependencies for each network
	for _, id := range ids {
		if err := networkValidator.CheckDependencies(ctx, id); err != nil {
			return err
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.DeleteNetworksByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete networks")
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	// Синхронизация с sgroups после успешного удаления из БД
	if len(networks) > 0 {
		log.Printf("🔧 DEBUG: DeleteNetworksByIDs - Starting sgroups synchronization for %d deleted Networks", len(networks))
		deleteSyncResults := s.syncNetworksWithSGroups(ctx, networks, types.SyncOperationDelete)
		log.Printf("✅ DEBUG: DeleteNetworksByIDs - Completed sgroups synchronization")

		// Логируем результаты синхронизации удаления для отладки
		for _, network := range networks {
			if err := deleteSyncResults[network.GetSyncKey()]; err != nil {
				log.Printf("❌ ERROR: DeleteNetworksByIDs - Network %s delete sync failed: %v", network.GetSyncKey(), err)
			}
		}
	}

	return nil
}

// DeleteNetworkBindingsByIDs deletes network bindings by IDs
func (s *NetguardService) DeleteNetworkBindingsByIDs(ctx context.Context, ids []models.ResourceIdentifier) error {
	reader, err := s.registry.Reader(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get reader")
	}
	defer reader.Close()

	// Create validator for dependency checking
	validator := validation.NewDependencyValidator(reader)
	bindingValidator := validator.GetNetworkBindingValidator()

	// Check dependencies for each network binding
	for _, id := range ids {
		if err := bindingValidator.CheckDependencies(ctx, id); err != nil {
			return err
		}
	}

	writer, err := s.registry.Writer(ctx)
	if err != nil {
		return errors.Wrap(err, "failed to get writer")
	}
	defer func() {
		if err != nil {
			writer.Abort()
		}
	}()

	if err = writer.DeleteNetworkBindingsByIDs(ctx, ids); err != nil {
		return errors.Wrap(err, "failed to delete network bindings")
	}

	if err = writer.Commit(); err != nil {
		return errors.Wrap(err, "failed to commit")
	}

	return nil
}

// servicePortsChanged проверяет, изменились ли порты между старой и новой версией сервиса
func (s *NetguardService) servicePortsChanged(oldService, newService models.Service) bool {
	// Сравниваем количество портов
	if len(oldService.IngressPorts) != len(newService.IngressPorts) {
		log.Printf("Service %s: port count changed from %d to %d", newService.Key(), len(oldService.IngressPorts), len(newService.IngressPorts))
		return true
	}

	// Создаем карту портов для быстрого сравнения
	oldPortsMap := make(map[string]models.TransportProtocol)
	for _, port := range oldService.IngressPorts {
		oldPortsMap[port.Port] = port.Protocol
	}

	// Проверяем, что все новые порты есть в старых с тем же протоколом
	for _, port := range newService.IngressPorts {
		if oldProtocol, exists := oldPortsMap[port.Port]; !exists || oldProtocol != port.Protocol {
			log.Printf("Service %s: port %s protocol changed from %s to %s",
				newService.Key(), port.Port, oldProtocol, port.Protocol)
			return true
		}
	}

	log.Printf("Service %s: ports unchanged", newService.Key())
	return false
}

// CompareServicePorts сравнивает порты между старой и новой версией сервиса и возвращает diff
func CompareServicePorts(oldPorts, newPorts []models.IngressPort) (added, removed []models.IngressPort) {
	oldPortSet := make(map[string]models.IngressPort)
	newPortSet := make(map[string]models.IngressPort)

	// Создаем карты портов для быстрого поиска (ключ: port/protocol)
	for _, port := range oldPorts {
		key := port.Port + "/" + string(port.Protocol)
		oldPortSet[key] = port
	}
	for _, port := range newPorts {
		key := port.Port + "/" + string(port.Protocol)
		newPortSet[key] = port
	}

	// Находим добавленные порты (есть в new, нет в old)
	for key, port := range newPortSet {
		if _, exists := oldPortSet[key]; !exists {
			added = append(added, port)
		}
	}

	// Находим удаленные порты (есть в old, нет в new)
	for key, port := range oldPortSet {
		if _, exists := newPortSet[key]; !exists {
			removed = append(removed, port)
		}
	}

	return added, removed
}

// ServicePortDiff содержит информацию о изменениях портов сервиса
type ServicePortDiff struct {
	ServiceID    models.ResourceIdentifier
	AddedPorts   []models.IngressPort
	RemovedPorts []models.IngressPort
	IsNewService bool
}

// ContributingRule представляет RuleS2S которая вносит вклад в агрегацию портов
type ContributingRule struct {
	RuleS2S *models.RuleS2S
	Ports   []models.IngressPort // Полная информация о портах с протоколами
}

// findContributingRuleS2S находит все RuleS2S которые должны вносить вклад в один IEAgAgRule
func (s *NetguardService) findContributingRuleS2S(
	ctx context.Context,
	reader ports.Reader,
	currentRule models.RuleS2S,
	localService models.Service,
	targetService models.Service,
) ([]ContributingRule, error) {
	klog.Infof("🔥 CONTRIBUTING_SEARCH: Starting search for rules contributing to %s", currentRule.Key())
	klog.Infof("    🔥 CURRENT SERVICES: local=%s target=%s", localService.Key(), targetService.Key())
	klog.Infof("    🔥 PROTECTION ENABLED: Will check if current service has ports to prevent obsolete rule creation!")
	klog.Infof("🔍 CONTRIBUTING: Finding contributing RuleS2S for rule %s", currentRule.Key())
	klog.Infof("🔍 CONTRIBUTING: localService=%s, targetService=%s", localService.Key(), targetService.Key())

	// Получаем все RuleS2S из базы данных
	var allRules []models.RuleS2S
	err := reader.ListRuleS2S(ctx, func(rule models.RuleS2S) error {
		allRules = append(allRules, rule)
		return nil
	}, nil)
	if err != nil {
		return nil, errors.Wrap(err, "failed to list RuleS2S")
	}

	klog.Infof("🔍 CONTRIBUTING: Found %d total RuleS2S in database", len(allRules))

	var contributingRules []ContributingRule
	var checkedRules, skippedRules, errorRules int

	for i, rule := range allRules {
		klog.Infof("  🔍 CHECKING[%d/%d]: Rule %s (%s %s→%s)",
			i+1, len(allRules), rule.Key(), rule.Traffic,
			rule.ServiceLocalRef.Key(), rule.ServiceRef.Key())

		checkedRules++
		contributes, ports, err := s.checkIfRuleContributes(ctx, reader, rule, currentRule, localService, targetService)
		if err != nil {
			errorRules++
			klog.Errorf("🚨 CONTRIBUTING: Error checking rule contribution for %s: %v", rule.Key(), err)
			continue
		}

		if contributes && len(ports) > 0 {
			var portStrs []string
			for _, p := range ports {
				portStrs = append(portStrs, fmt.Sprintf("%s/%s", p.Port, p.Protocol))
			}
			klog.Infof("🎯 CONTRIBUTING: Found contributing rule %s with %d ports: %s",
				rule.Key(), len(ports), strings.Join(portStrs, ","))

			contributingRules = append(contributingRules, ContributingRule{
				RuleS2S: &rule,
				Ports:   ports,
			})
		} else {
			skippedRules++
			if contributes {
				klog.Infof("  ⚠️ CONTRIBUTING: Rule %s contributes but has 0 ports", rule.Key())
			} else {
				klog.Infof("  ➖ CONTRIBUTING: Rule %s does not contribute", rule.Key())
			}
		}
	}

	klog.Infof("🔍 CONTRIBUTING: Summary for %s: checked=%d, contributing=%d, skipped=%d, errors=%d",
		currentRule.Key(), checkedRules, len(contributingRules), skippedRules, errorRules)
	return contributingRules, nil
}

// generateAGPairsForRule генерирует все AG пары для RuleS2S
func (s *NetguardService) generateAGPairsForRule(
	traffic models.Traffic,
	localService models.Service,
	targetService models.Service,
	protocol models.TransportProtocol,
) []string {
	klog.Infof("🔧 AG_PAIRS: Generating AG pairs for %s local=%s(%d AGs) target=%s(%d AGs) protocol=%s",
		traffic, localService.Key(), len(localService.AddressGroups),
		targetService.Key(), len(targetService.AddressGroups), protocol)

	var agPairs []string
	for i, localAG := range localService.AddressGroups {
		for j, targetAG := range targetService.AddressGroups {
			agPair := fmt.Sprintf("%s|%s|%s|%s", traffic, localAG.Key(), targetAG.Key(), protocol)
			agPairs = append(agPairs, agPair)
			klog.Infof("  🔧 AG_PAIRS[%d,%d]: %s → %s = %s",
				i, j, localAG.Key(), targetAG.Key(), agPair)
		}
	}

	klog.Infof("🔧 AG_PAIRS: Generated %d AG pairs total", len(agPairs))
	return agPairs
}

// checkIfRuleContributes проверяет должна ли RuleS2S вносить вклад в агрегацию по AG парам!
func (s *NetguardService) checkIfRuleContributes(
	ctx context.Context,
	reader ports.Reader,
	candidateRule models.RuleS2S,
	currentRule models.RuleS2S,
	localService models.Service,
	targetService models.Service,
) (bool, []models.IngressPort, error) {
	klog.Infof("🔍 CONTRIBUTES: Checking if %s contributes to %s", candidateRule.Key(), currentRule.Key())

	// ✅ Проверка 1: Одинаковое направление трафика
	if candidateRule.Traffic != currentRule.Traffic {
		klog.Infof("  ❌ CONTRIBUTES: Different traffic direction: %s vs %s", candidateRule.Traffic, currentRule.Traffic)
		return false, nil, nil
	}
	klog.Infof("  ✅ CONTRIBUTES: Same traffic direction: %s", candidateRule.Traffic)

	// ✅ Проверка 2: Получаем сервисы для candidate rule
	candidateLocalService, candidateTargetService, err := s.getServicesForRule(ctx, reader, candidateRule)
	if err != nil {
		klog.Errorf("  🚨 CONTRIBUTES: Failed to get services for %s: %v", candidateRule.Key(), err)
		return false, nil, err
	}
	klog.Infof("  ✅ CONTRIBUTES: Got candidate services: local=%s target=%s",
		candidateLocalService.Key(), candidateTargetService.Key())

	// ✅ НОВАЯ ЛОГИКА: Проверяем пересечение AG пар вместо сравнения сервисов!
	klog.Infof("  🔧 CONTRIBUTES: Generating AG pairs for comparison...")

	// Генерируем AG пары для currentRule (для любого протокола - используем TCP как базовый)
	klog.Infof("    🔧 CURRENT_RULE AG pairs:")
	currentAGPairs := s.generateAGPairsForRule(currentRule.Traffic, localService, targetService, models.TCP)

	// Генерируем AG пары для candidateRule
	klog.Infof("    🔧 CANDIDATE_RULE AG pairs:")
	candidateAGPairs := s.generateAGPairsForRule(candidateRule.Traffic, *candidateLocalService, *candidateTargetService, models.TCP)

	// Проверяем есть ли пересечения в AG парах (убираем протокол из сравнения)
	klog.Infof("  🔍 CONTRIBUTES: Checking AG pairs intersection...")
	currentPairsSet := make(map[string]bool)
	for i, pair := range currentAGPairs {
		// Убираем протокол из ключа для сравнения (заменяем "|TCP" на "")
		pairWithoutProtocol := strings.Replace(pair, "|TCP", "", 1)
		currentPairsSet[pairWithoutProtocol] = true
		klog.Infof("    🔍 CURRENT_PAIRS[%d]: %s → without protocol: %s", i, pair, pairWithoutProtocol)
	}

	hasIntersection := false
	var intersectedPairs []string
	for i, pair := range candidateAGPairs {
		// Убираем протокол из ключа для сравнения
		pairWithoutProtocol := strings.Replace(pair, "|TCP", "", 1)
		klog.Infof("    🔍 CANDIDATE_PAIRS[%d]: %s → without protocol: %s", i, pair, pairWithoutProtocol)

		if currentPairsSet[pairWithoutProtocol] {
			hasIntersection = true
			intersectedPairs = append(intersectedPairs, pairWithoutProtocol)
			klog.Infof("      ✅ INTERSECTION FOUND: %s", pairWithoutProtocol)
		} else {
			klog.Infof("      ❌ NO INTERSECTION: %s", pairWithoutProtocol)
		}
	}

	// Если нет пересечений AG пар - не contributing
	if !hasIntersection {
		klog.Infof("  ❌ CONTRIBUTES: No AG pairs intersection found → NOT contributing")
		return false, nil, nil
	}

	klog.Infof("  ✅ CONTRIBUTES: Found %d AG pairs intersections: %v → IS contributing",
		len(intersectedPairs), intersectedPairs)

	// 🔥 ПРОВЕРКА 3: КРИТИЧНО - проверяем что ТЕКУЩИЙ сервис имеет порты!
	klog.Infof("  🔍 CONTRIBUTES: Checking CURRENT service ports to prevent obsolete rule creation...")

	var currentServicePorts []models.IngressPort
	var currentServiceName string
	if currentRule.Traffic == models.INGRESS {
		currentServicePorts = s.extractPortsFromService(localService)
		currentServiceName = localService.Key()
		klog.Infof("    🔍 CURRENT RULE: %s uses LOCAL service %s", currentRule.Key(), currentServiceName)
	} else {
		currentServicePorts = s.extractPortsFromService(targetService)
		currentServiceName = targetService.Key()
		klog.Infof("    🔍 CURRENT RULE: %s uses TARGET service %s", currentRule.Key(), currentServiceName)
	}

	var currentPortStrs []string
	for _, p := range currentServicePorts {
		currentPortStrs = append(currentPortStrs, fmt.Sprintf("%s/%s", p.Port, p.Protocol))
	}
	klog.Infof("    🔍 CURRENT SERVICE: %s has %d ports: %v",
		currentServiceName, len(currentServicePorts), currentPortStrs)

	// 🚨 СМЯГЧЕННАЯ ПРОВЕРКА: Логируем текущий сервис но НЕ блокируем contributing rules
	if len(currentServicePorts) == 0 {
		klog.Infof("  ⚠️ CONTRIBUTES: Current service %s has 0 ports (will be filtered in aggregation)", currentServiceName)
	} else {
		klog.Infof("  ✅ CONTRIBUTES: Current service %s has %d ports → can contribute",
			currentServiceName, len(currentServicePorts))
	}

	// 🔥 ПРОВЕРКА 4: Извлекаем порты из candidate rule для агрегации
	klog.Infof("  🔍 CONTRIBUTES: Now checking CANDIDATE service ports for aggregation...")

	var candidatePorts []models.IngressPort
	var candidateServiceName string
	if candidateRule.Traffic == models.INGRESS {
		candidatePorts = s.extractPortsFromService(*candidateLocalService)
		candidateServiceName = candidateLocalService.Key()
		klog.Infof("    🔍 CANDIDATE RULE: %s uses LOCAL service %s", candidateRule.Key(), candidateServiceName)
	} else {
		candidatePorts = s.extractPortsFromService(*candidateTargetService)
		candidateServiceName = candidateTargetService.Key()
		klog.Infof("    🔍 CANDIDATE RULE: %s uses TARGET service %s", candidateRule.Key(), candidateServiceName)
	}

	var candidatePortStrs []string
	for _, p := range candidatePorts {
		candidatePortStrs = append(candidatePortStrs, fmt.Sprintf("%s/%s", p.Port, p.Protocol))
	}
	klog.Infof("    🔍 CANDIDATE SERVICE: %s has %d ports: %v",
		candidateServiceName, len(candidatePorts), candidatePortStrs)

	// 🚨 Если candidate тоже не имеет портов → не contributing
	if len(candidatePorts) == 0 {
		klog.Infof("  ❌ CONTRIBUTES: Candidate service %s has 0 ports → NOT contributing", candidateServiceName)
		return false, nil, nil
	}

	klog.Infof("  ✅ CONTRIBUTES: Candidate service %s has %d ports → can contribute",
		candidateServiceName, len(candidatePorts))

	// 🎯 ФИНАЛЬНОЕ РЕШЕНИЕ
	klog.Infof("🎯 CONTRIBUTES: FINAL DECISION for rule %s:", candidateRule.Key())
	klog.Infof("    ✅ AG pairs intersect: %v", intersectedPairs)
	klog.Infof("    ✅ Current service %s has %d ports", currentServiceName, len(currentServicePorts))
	klog.Infof("    ✅ Candidate service %s has %d ports", candidateServiceName, len(candidatePorts))
	klog.Infof("    ✅ Will contribute %d ports from %s: %s",
		len(candidatePorts), candidateServiceName, strings.Join(candidatePortStrs, ","))

	return true, candidatePorts, nil
}

// getServicesForRule получает сервисы для RuleS2S (аналогично K8s контроллеру)
func (s *NetguardService) getServicesForRule(
	ctx context.Context,
	reader ports.Reader,
	rule models.RuleS2S,
) (*models.Service, *models.Service, error) {
	// Получаем local service через ServiceAlias
	localServiceAlias, err := reader.GetServiceAliasByID(ctx, rule.ServiceLocalRef.ResourceIdentifier)
	if err != nil {
		return nil, nil, errors.Wrapf(err, "failed to get local service alias %s", rule.ServiceLocalRef.Key())
	}

	localService, err := reader.GetServiceByID(ctx, localServiceAlias.ServiceRef.ResourceIdentifier)
	if err != nil {
		return nil, nil, errors.Wrapf(err, "failed to get local service %s", localServiceAlias.ServiceRef.Key())
	}

	// Получаем target service через ServiceAlias
	targetServiceAlias, err := reader.GetServiceAliasByID(ctx, rule.ServiceRef.ResourceIdentifier)
	if err != nil {
		return nil, nil, errors.Wrapf(err, "failed to get target service alias %s", rule.ServiceRef.Key())
	}

	targetService, err := reader.GetServiceByID(ctx, targetServiceAlias.ServiceRef.ResourceIdentifier)
	if err != nil {
		return nil, nil, errors.Wrapf(err, "failed to get target service %s", targetServiceAlias.ServiceRef.Key())
	}

	return localService, targetService, nil
}

// servicesHaveSameAddressGroups проверяет имеют ли сервисы одинаковые AddressGroups
func (s *NetguardService) servicesHaveSameAddressGroups(
	service1 models.Service,
	service2 models.Service,
) bool {
	if len(service1.AddressGroups) != len(service2.AddressGroups) {
		return false
	}

	agMap := make(map[string]bool)
	for _, ag := range service1.AddressGroups {
		key := ag.Key()
		agMap[key] = true
	}

	for _, ag := range service2.AddressGroups {
		key := ag.Key()
		if !agMap[key] {
			return false
		}
	}

	return true
}

// extractPortsFromService извлекает порты из сервиса с полной информацией о протоколах
func (s *NetguardService) extractPortsFromService(service models.Service) []models.IngressPort {
	return service.IngressPorts
}

// aggregatePortsWithProtocol агрегирует порты для конкретного протокола
func (s *NetguardService) aggregatePortsWithProtocol(
	contributingRules []ContributingRule,
	protocol models.TransportProtocol,
) []string {
	klog.Infof("🔀 PORT_AGGREGATION: Starting aggregation for protocol %s with %d contributing rules",
		protocol, len(contributingRules))

	portSet := make(map[string]bool)

	for i, rule := range contributingRules {
		// Фильтруем порты по протоколу
		var protocolPorts []string
		for _, port := range rule.Ports {
			if port.Protocol == protocol {
				protocolPorts = append(protocolPorts, port.Port)
			}
		}

		klog.Infof("  🔀 RULE[%d]: %s has %d total ports, %d %s ports: %v",
			i, rule.RuleS2S.Key(), len(rule.Ports), len(protocolPorts), protocol, protocolPorts)

		for j, port := range protocolPorts {
			portSet[port] = true
			klog.Infof("    🔀 PORT[%d]: %s → added to aggregation", j, port)
		}
	}

	var aggregatedPorts []string
	for port := range portSet {
		aggregatedPorts = append(aggregatedPorts, port)
	}

	sort.Strings(aggregatedPorts)

	klog.Infof("🎯 PORT_AGGREGATION: Final aggregated ports for %s: %d ports → %v",
		protocol, len(aggregatedPorts), aggregatedPorts)

	return aggregatedPorts
}
